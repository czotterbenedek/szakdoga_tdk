{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Online clustering\n",
    "\n",
    "this the notebook which contains the code for online clustering the embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tdk_szakdoga/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "\n",
    "from sklearn.cluster import DBSCAN, KMeans, AgglomerativeClustering\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import silhouette_score\n",
    "import umap.umap_ as umap\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnlineKMeans:\n",
    "    def __init__(\n",
    "        self,\n",
    "        n_clusters=4,\n",
    "        max_clusters=20,\n",
    "        metric=\"euclidean\",   # \"euclidean\" or \"cosine\"\n",
    "        new_cluster_threshold=None, # if None, no dynamic creation\n",
    "        merge_threshold=None,  # if not None, merge centroids closer than this\n",
    "        decay=1.0,             # multiply counts by this factor periodically (<=1.0)\n",
    "        random_state=None,\n",
    "    ):\n",
    "        self.n_clusters = n_clusters\n",
    "        self.max_clusters = max_clusters\n",
    "        self.metric = metric\n",
    "        self.new_cluster_threshold = new_cluster_threshold\n",
    "        self.merge_threshold = merge_threshold\n",
    "        self.decay = decay\n",
    "        self.rng = np.random.RandomState(random_state)\n",
    "\n",
    "        self.centroids = None     # (k, d)\n",
    "        self.counts = None        # (k,)\n",
    "        self.sums = None          # (k, d) running sums (optional convenience)\n",
    "        self.vars = None          # per-cluster variance estimate (scalar)\n",
    "        self.total_seen = 0\n",
    "\n",
    "    def _normalize(self, X):\n",
    "        norms = np.linalg.norm(X, axis=1, keepdims=True)\n",
    "        norms[norms == 0] = 1.0\n",
    "        return X / norms\n",
    "\n",
    "    def _pairwise_dist(self, X, C):\n",
    "        # returns (n_points, n_centroids) distances\n",
    "        if self.metric == \"euclidean\":\n",
    "            # squared distance then sqrt\n",
    "            d2 = np.sum((X[:, None, :] - C[None, :, :]) ** 2, axis=2)\n",
    "            return np.sqrt(d2 + 1e-12)\n",
    "        else:  # cosine distance = 1 - dot(normalized)\n",
    "            Xn = self._normalize(X)\n",
    "            Cn = self._normalize(C)\n",
    "            return 1.0 - (Xn @ Cn.T)\n",
    "\n",
    "    def _kmeans_pp_init(self, X, k):\n",
    "        # returns k centers chosen from X (indices) by kmeans++\n",
    "        n = X.shape[0]\n",
    "        centers = []\n",
    "        first = self.rng.randint(0, n)\n",
    "        centers.append(first)\n",
    "        d2 = np.full(n, np.inf)\n",
    "        for _ in range(1, k):\n",
    "            cur = X[centers[-1 :], :]\n",
    "            dist = np.sum((X - cur) ** 2, axis=1)\n",
    "            d2 = np.minimum(d2, dist)\n",
    "            probs = d2 / (d2.sum() + 1e-12)\n",
    "            next_idx = self.rng.choice(n, p=probs)\n",
    "            centers.append(int(next_idx))\n",
    "        return np.array(centers, dtype=int)\n",
    "\n",
    "    def initialize_centroids(self, X_init):\n",
    "        # initialize centroids using kmeans++ on X_init (or random fallback)\n",
    "        k = self.n_clusters\n",
    "        if X_init.shape[0] < k:\n",
    "            # fallback: random pick with replacement\n",
    "            idx = self.rng.choice(len(X_init), k, replace=True)\n",
    "            centers = X_init[idx]\n",
    "        else:\n",
    "            idx = self._kmeans_pp_init(X_init, k)\n",
    "            centers = X_init[idx]\n",
    "        if self.metric == \"cosine\":\n",
    "            centers = self._normalize(centers)\n",
    "        self.centroids = centers.copy()\n",
    "        self.counts = np.zeros(len(self.centroids), dtype=float)\n",
    "        self.sums = np.zeros_like(self.centroids)\n",
    "        self.vars = np.zeros(len(self.centroids), dtype=float)\n",
    "        self.total_seen = 0\n",
    "\n",
    "    def partial_fit(self, X_batch):\n",
    "        \"\"\"\n",
    "        Vectorized minibatch update:\n",
    "         - assign points to nearest centroid\n",
    "         - for each cluster, compute batch sum and batch count\n",
    "         - do exact incremental mean update using counts\n",
    "        Also can create new clusters for far-away points (optional).\n",
    "        \"\"\"\n",
    "        X = np.asarray(X_batch, dtype=float)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "\n",
    "        if self.metric == \"cosine\":\n",
    "            X = self._normalize(X)\n",
    "\n",
    "        if self.centroids is None:\n",
    "            # init from first batch\n",
    "            self.initialize_centroids(X)\n",
    "            # if we used kmeans++ the centroids are populated but counts=0\n",
    "\n",
    "        # compute pairwise distances\n",
    "        D = self._pairwise_dist(X, self.centroids)\n",
    "        min_dist = D.min(axis=1)\n",
    "        labels = D.argmin(axis=1)\n",
    "\n",
    "        # dynamic creation of new clusters for far-away points\n",
    "        if (self.new_cluster_threshold is not None) and (len(self.centroids) < self.max_clusters):\n",
    "            far_idx = np.where(min_dist > self.new_cluster_threshold)[0]\n",
    "            # create up to capacity new clusters from these points\n",
    "            to_create = min(len(far_idx), self.max_clusters - len(self.centroids))\n",
    "            for j in range(to_create):\n",
    "                i = far_idx[j]\n",
    "                new_center = X[i].copy()\n",
    "                if self.metric == \"cosine\":\n",
    "                    new_center = self._normalize(new_center.reshape(1, -1))[0]\n",
    "                # append new centroid\n",
    "                self.centroids = np.vstack([self.centroids, new_center])\n",
    "                self.counts = np.concatenate([self.counts, np.array([0.0])])\n",
    "                self.sums = np.vstack([self.sums, np.zeros_like(new_center)])\n",
    "                self.vars = np.concatenate([self.vars, np.array([0.0])])\n",
    "                # reassign label of this point to the new cluster\n",
    "                labels[i] = len(self.centroids) - 1\n",
    "\n",
    "            # recompute distances/labels if centers changed (safe)\n",
    "            D = self._pairwise_dist(X, self.centroids)\n",
    "            min_dist = D.min(axis=1)\n",
    "            labels = D.argmin(axis=1)\n",
    "\n",
    "        # vectorized update: for each cluster compute batch_sum and m\n",
    "        k_now = len(self.centroids)\n",
    "        for k in range(k_now):\n",
    "            mask = labels == k\n",
    "            m = mask.sum()\n",
    "            if m == 0:\n",
    "                continue\n",
    "            batch_points = X[mask]\n",
    "            batch_sum = batch_points.sum(axis=0)\n",
    "            batch_mean = batch_sum / m\n",
    "\n",
    "            # incremental mean update using counts\n",
    "            n_old = self.counts[k]\n",
    "            if n_old == 0:\n",
    "                # pure initialization for this cluster\n",
    "                self.centroids[k] = batch_mean\n",
    "                self.counts[k] = m\n",
    "                self.sums[k] = batch_sum\n",
    "                # compute variance (scalar average squared distance)\n",
    "                diffs = batch_points - batch_mean\n",
    "                self.vars[k] = np.mean(np.sum(diffs ** 2, axis=1))\n",
    "            else:\n",
    "                new_count = n_old + m\n",
    "                # update centroid to combined mean:\n",
    "                new_centroid = (n_old * self.centroids[k] + batch_sum) / new_count\n",
    "                # update variance with combined formula:\n",
    "                # var_new = (n_old*(var_old + (mu_old - mu_new)^2) + m*(var_batch + (mu_batch-mu_new)^2)) / new_count\n",
    "                var_old = self.vars[k]\n",
    "                mu_old = self.centroids[k]\n",
    "                mu_batch = batch_mean\n",
    "                # batch variance\n",
    "                diffs = batch_points - mu_batch\n",
    "                var_batch = np.mean(np.sum(diffs ** 2, axis=1))\n",
    "                delta_old = mu_old - new_centroid\n",
    "                delta_batch = mu_batch - new_centroid\n",
    "                var_new = (n_old * (var_old + np.sum(delta_old ** 2)) + m * (var_batch + np.sum(delta_batch ** 2))) / new_count\n",
    "\n",
    "                # commit updates\n",
    "                self.centroids[k] = new_centroid\n",
    "                self.counts[k] = new_count\n",
    "                self.sums[k] += batch_sum\n",
    "                self.vars[k] = var_new\n",
    "\n",
    "        self.total_seen += len(X)\n",
    "\n",
    "        # renormalize centroids for cosine\n",
    "        if self.metric == \"cosine\":\n",
    "            self.centroids = self._normalize(self.centroids)\n",
    "\n",
    "        # optional: merge very close clusters\n",
    "        if (self.merge_threshold is not None) and (len(self.centroids) > 1):\n",
    "            self._merge_close_clusters()\n",
    "\n",
    "    def _merge_close_clusters(self):\n",
    "        # merges centroids that are closer than merge_threshold\n",
    "        C = self.centroids\n",
    "        k = len(C)\n",
    "        D = self._pairwise_dist(C, C)  # symmetric\n",
    "        np.fill_diagonal(D, np.inf)\n",
    "        merge_pairs = np.argwhere(D < self.merge_threshold)\n",
    "        if len(merge_pairs) == 0:\n",
    "            return\n",
    "        to_remove = set()\n",
    "        for i, j in merge_pairs:\n",
    "            if i in to_remove or j in to_remove:\n",
    "                continue\n",
    "            # merge j into i (weighted)\n",
    "            n_i = self.counts[i]\n",
    "            n_j = self.counts[j]\n",
    "            total = n_i + n_j if (n_i + n_j) > 0 else 1.0\n",
    "            new_centroid = (n_i * self.centroids[i] + n_j * self.centroids[j]) / total\n",
    "            self.centroids[i] = new_centroid\n",
    "            # update counts, sums, variance\n",
    "            self.counts[i] = total\n",
    "            self.sums[i] = self.sums[i] + self.sums[j]\n",
    "            self.vars[i] = (self.vars[i] + self.vars[j]) / 2.0\n",
    "            to_remove.add(j)\n",
    "        if to_remove:\n",
    "            keep = [idx for idx in range(len(self.centroids)) if idx not in to_remove]\n",
    "            self.centroids = self.centroids[keep]\n",
    "            self.counts = self.counts[keep]\n",
    "            self.sums = self.sums[keep]\n",
    "            self.vars = self.vars[keep]\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.asarray(X, dtype=float)\n",
    "        if X.ndim == 1:\n",
    "            X = X.reshape(1, -1)\n",
    "        if self.metric == \"cosine\":\n",
    "            X = self._normalize(X)\n",
    "        D = self._pairwise_dist(X, self.centroids)\n",
    "        return D.argmin(axis=1)\n",
    "\n",
    "    def get_state(self):\n",
    "        return {\n",
    "            \"centroids\": self.centroids.copy(),\n",
    "            \"counts\": self.counts.copy(),\n",
    "            \"vars\": self.vars.copy(),\n",
    "            \"total_seen\": self.total_seen,\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_semantic_train = np.load(\"./data/tensors/squad_train_v2_semantic_chunking.npy\")\n",
    "df_semantic_train = pd.read_excel(\"./data/prepared/squad_train_v2_semantic_chunking.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_docs = X_semantic_train.shape[0]\n",
    "split_idx = num_docs // 2\n",
    "embeddings_init = X_semantic_train[:split_idx]\n",
    "embeddings_online = X_semantic_train[split_idx:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "online_kmeans = OnlineKMeans(\n",
    "    n_clusters=500,               # start with 50 clusters (tune as needed)\n",
    "    max_clusters=2000,            # allow max 200 clusters\n",
    "    metric=\"cosine\",             # cosine similarity is common for embeddings\n",
    "    new_cluster_threshold=0.8,   # create new clusters for far away points\n",
    "    merge_threshold=0.08,        # merge clusters that are very close\n",
    "    decay=1.0,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Initialize with first half of the data\n",
    "online_kmeans.initialize_centroids(embeddings_init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "initial_centroids = online_kmeans.centroids.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optionally: assign initial cluster labels\n",
    "initial_labels = online_kmeans.predict(embeddings_init)\n",
    "df_semantic_train.loc[:split_idx-1, 'cluster'] = initial_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 165/165 [00:01<00:00, 140.40it/s]\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Process the rest incrementally\n",
    "batch_size = 256\n",
    "for start in tqdm(range(0, embeddings_online.shape[0], batch_size)):\n",
    "    end = min(start + batch_size, embeddings_online.shape[0])\n",
    "    batch = embeddings_online[start:end]\n",
    "    online_kmeans.partial_fit(batch)\n",
    "    labels = online_kmeans.predict(batch)\n",
    "    df_semantic_train.loc[split_idx + start: split_idx + end - 1, 'cluster'] = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "533"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_semantic_train['cluster'].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdk_szakdoga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
