{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/tdk_szakdoga/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from OnlineKMeans import OnlineKMeans\n",
    "import faiss\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_answer_in_chunk(answer_start, chunk_start, chunk_length):\n",
    "    if answer_start is None or chunk_start is None or chunk_length is None:\n",
    "        return False\n",
    "    return chunk_start <= answer_start < (chunk_start + chunk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_chunks_faiss(\n",
    "    query_embedding,\n",
    "    df_chunks,\n",
    "    chunk_embeddings,\n",
    "    faiss_index,\n",
    "    top_k=5\n",
    "):\n",
    "    \"\"\"\n",
    "    Retrieve top-k chunks using a FAISS index\n",
    "    \"\"\"\n",
    "    query_vec = np.expand_dims(query_embedding.astype(np.float32), axis=0)\n",
    "    sims, idxs = faiss_index.search(query_vec, top_k)\n",
    "    \n",
    "    results = []\n",
    "    for sim, idx in zip(sims[0], idxs[0]):\n",
    "        results.append({\n",
    "            \"context_id\": df_chunks.iloc[idx][\"context_id\"],\n",
    "            \"chunk_id\": df_chunks.iloc[idx][\"chunk_id\"],\n",
    "            \"title\": df_chunks.iloc[idx][\"title\"],\n",
    "            \"chunk_embed_text\": df_chunks.iloc[idx][\"chunk_embed_text\"],\n",
    "            \"chunk_start\": df_chunks.iloc[idx][\"chunk_start\"],\n",
    "            \"chunk_end\": df_chunks.iloc[idx][\"chunk_end\"],\n",
    "            \"similarity\": sim\n",
    "        })\n",
    "    return pd.DataFrame(results).sort_values(\"similarity\", ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_top_k_accuracy_faiss(\n",
    "    df_queries,\n",
    "    df_chunks,\n",
    "    chunk_embeddings,\n",
    "    faiss_index,\n",
    "    top_k=5\n",
    "):\n",
    "    y_true_doc, y_pred_doc = [], []\n",
    "    y_true_chunk, y_pred_chunk = [], []\n",
    "\n",
    "    chunk_ratios = []\n",
    "\n",
    "    for _, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "        query_emb = model.encode([row[\"question\"]])[0]\n",
    "        results = retrieve_top_chunks_faiss(\n",
    "            query_embedding=query_emb,\n",
    "            df_chunks=df_chunks,\n",
    "            chunk_embeddings=chunk_embeddings,\n",
    "            faiss_index=faiss_index,\n",
    "            top_k=top_k\n",
    "        )\n",
    "\n",
    "        # Document-level\n",
    "        found_doc_id = any(row[\"context_id\"] == doc_id for doc_id in results[\"context_id\"])\n",
    "        y_true_doc.append(1)\n",
    "        y_pred_doc.append(1 if found_doc_id else 0)\n",
    "\n",
    "        correct_doc_chunks = results[results[\"context_id\"] == row[\"context_id\"]]\n",
    "        found_chunk_context = any(\n",
    "            is_answer_in_chunk(\n",
    "                row[\"answer_start\"],\n",
    "                chunk[\"chunk_start\"],\n",
    "                chunk[\"chunk_end\"] - chunk[\"chunk_start\"]\n",
    "            )\n",
    "            for _, chunk in correct_doc_chunks.iterrows()\n",
    "        )\n",
    "        good_chunks = len(correct_doc_chunks)\n",
    "        total_chunks = results.shape[0]\n",
    "        ratio = good_chunks / total_chunks\n",
    "        chunk_ratios.append(ratio)\n",
    "\n",
    "        y_true_chunk.append(1)\n",
    "        y_pred_chunk.append(1 if found_chunk_context else 0)\n",
    "\n",
    "    # Compute metrics\n",
    "    chunk_accuracy = sum(chunk_ratios) / len(chunk_ratios) if len(chunk_ratios) > 0 else 0\n",
    "    metrics = {\n",
    "        \"doc_accuracy\": sum(y_pred_doc) / len(y_pred_doc),\n",
    "        \"chunk_accuracy\": sum(y_pred_chunk) / len(y_pred_chunk),\n",
    "        \"doc_precision\": precision_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "        \"doc_recall\": recall_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "        \"doc_f1\": f1_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "        \"chunk_precision\": precision_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "        \"chunk_recall\": recall_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "        \"chunk_f1\": f1_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "        \"correct_chunk_accuracy\": chunk_accuracy\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def online_kmeans_faiss_retrieval(\n",
    "#     chunk_embeddings,\n",
    "#     df_chunks,\n",
    "#     df_queries,\n",
    "#     n_clusters=500,\n",
    "#     batch_size=2000,\n",
    "#     top_k=5,\n",
    "#     init_fraction=0.5,\n",
    "#     max_clusters=None,\n",
    "#     metric=\"cosine\",\n",
    "#     new_cluster_threshold=None,\n",
    "#     merge_threshold=None,\n",
    "#     decay=None\n",
    "# ):\n",
    "#     n_samples = chunk_embeddings.shape[0]\n",
    "#     init_size = int(n_samples * init_fraction)\n",
    "#     remaining_size = n_samples - init_size\n",
    "\n",
    "#     # --- Step 1: Initialize OnlineKMeans ---\n",
    "#     print(f\"🔧 Using {init_fraction*100:.0f}% of data ({init_size} samples) for initialization\")\n",
    "#     okm = OnlineKMeans(\n",
    "#         n_clusters=n_clusters,\n",
    "#         max_clusters=max_clusters,\n",
    "#         metric=metric,\n",
    "#         new_cluster_threshold=new_cluster_threshold,\n",
    "#         merge_threshold=merge_threshold,\n",
    "#         random_state=42,\n",
    "#         decay=decay\n",
    "#     )\n",
    "#     init_start = time.time()\n",
    "#     okm.partial_fit(chunk_embeddings[:init_size])\n",
    "#     init_end = time.time()\n",
    "#     init_time = init_end - init_start\n",
    "#     print(f\"✅ Initialization done in {init_time:.4f} s\")\n",
    "\n",
    "#     results = []\n",
    "\n",
    "#     # --- Step 2: Online batch updates ---\n",
    "#     for batch_idx in tqdm(range(1, int(np.ceil(remaining_size / batch_size)) + 1)):\n",
    "#         start_idx = (batch_idx - 1) * batch_size\n",
    "#         end_idx = min(batch_idx * batch_size, remaining_size)\n",
    "#         batch_embeddings = chunk_embeddings[init_size + start_idx : init_size + end_idx].astype(np.float32, copy=False)\n",
    "#         batch_embeddings = np.ascontiguousarray(batch_embeddings)\n",
    "\n",
    "#         # --- Online update ---\n",
    "#         update_start = time.time()\n",
    "#         okm.partial_fit(batch_embeddings)\n",
    "#         update_end = time.time()\n",
    "#         update_time = update_end - update_start\n",
    "\n",
    "#         # --- Seen data so far ---\n",
    "#         seen_end_idx = init_size + end_idx\n",
    "#         seen_embeddings = chunk_embeddings[:seen_end_idx].astype(np.float32, copy=False)\n",
    "#         seen_df_chunks = df_chunks.iloc[:seen_end_idx].reset_index(drop=True)\n",
    "\n",
    "#         # --- Build FAISS index ---\n",
    "#         d = seen_embeddings.shape[1]\n",
    "#         if metric == \"cosine\":\n",
    "#             faiss_index = faiss.IndexFlatIP(d)  # inner product for cosine similarity\n",
    "#             faiss.normalize_L2(seen_embeddings)\n",
    "#         else:\n",
    "#             faiss_index = faiss.IndexFlatL2(d)\n",
    "#         faiss_index.add(seen_embeddings)\n",
    "\n",
    "#         # --- Filter queries ---\n",
    "#         df_queries_seen = df_queries[df_queries[\"context_id\"].isin(seen_df_chunks[\"context_id\"].unique())].reset_index(drop=True)\n",
    "\n",
    "#         # --- Evaluate retrieval ---\n",
    "#         retrieval_start = time.time()\n",
    "#         metrics = evaluate_top_k_accuracy_faiss(\n",
    "#             df_queries=df_queries_seen,\n",
    "#             df_chunks=seen_df_chunks,\n",
    "#             chunk_embeddings=seen_embeddings,\n",
    "#             faiss_index=faiss_index,\n",
    "#             top_k=top_k\n",
    "#         )\n",
    "#         retrieval_end = time.time()\n",
    "#         retrieval_time = retrieval_end - retrieval_start\n",
    "\n",
    "#         results.append({\n",
    "#             \"batch\": batch_idx,\n",
    "#             \"init_time\": init_time if batch_idx == 1 else 0,\n",
    "#             \"update_time\": update_time,\n",
    "#             \"retrieval_time\": retrieval_time,\n",
    "#             \"metrics\": metrics,\n",
    "#             \"n_clusters\": len(okm.centroids)\n",
    "#         })\n",
    "\n",
    "#         print(f\"[Batch {batch_idx}] Seen chunks: {seen_end_idx}, Doc acc: {metrics['doc_accuracy']:.4f}, Chunk acc: {metrics['chunk_accuracy']:.4f}, Clusters: {len(okm.centroids)}\")\n",
    "\n",
    "#     return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_kmeans_faiss_retrieval(\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    df_queries,\n",
    "    n_clusters=500,\n",
    "    batch_size=2000,\n",
    "    top_k=5,\n",
    "    init_fraction=0.5,\n",
    "    max_clusters=None,\n",
    "    metric=\"cosine\",\n",
    "    new_cluster_threshold=None,\n",
    "    merge_threshold=None,\n",
    "    decay=None\n",
    "):\n",
    "    n_samples = chunk_embeddings.shape[0]\n",
    "    init_size = int(n_samples * init_fraction)\n",
    "    remaining_size = n_samples - init_size\n",
    "\n",
    "    # --- Step 1: Initialize OnlineKMeans ---\n",
    "    print(f\"🔧 Using {init_fraction*100:.0f}% of data ({init_size} samples) for initialization\")\n",
    "    okm = OnlineKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        max_clusters=max_clusters,\n",
    "        metric=metric,\n",
    "        new_cluster_threshold=new_cluster_threshold,\n",
    "        merge_threshold=merge_threshold,\n",
    "        random_state=42,\n",
    "        decay=decay\n",
    "    )\n",
    "    init_start = time.time()\n",
    "    init_embeddings = chunk_embeddings[:init_size].astype(np.float32)\n",
    "    okm.partial_fit(init_embeddings)\n",
    "    init_end = time.time()\n",
    "    init_time = init_end - init_start\n",
    "    print(f\"✅ Initialization done in {init_time:.4f} s\")\n",
    "\n",
    "    # --- Step 2: Create FAISS index once ---\n",
    "    d = chunk_embeddings.shape[1]\n",
    "    if metric == \"cosine\":\n",
    "        faiss_index = faiss.IndexFlatIP(d)\n",
    "        faiss.normalize_L2(init_embeddings)\n",
    "    else:\n",
    "        faiss_index = faiss.IndexFlatL2(d)\n",
    "\n",
    "    faiss_index.add(init_embeddings)\n",
    "\n",
    "    # Mapping from FAISS index IDs → chunk/context IDs\n",
    "    index_to_chunk = df_chunks.iloc[:init_size][\"context_id\"].tolist()\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # --- Step 3: Online batch updates ---\n",
    "    for batch_idx in tqdm(range(1, int(np.ceil(remaining_size / batch_size)) + 1)):\n",
    "        start_idx = (batch_idx - 1) * batch_size\n",
    "        end_idx = min(batch_idx * batch_size, remaining_size)\n",
    "        batch_embeddings = chunk_embeddings[init_size + start_idx : init_size + end_idx].astype(np.float32)\n",
    "        batch_embeddings = np.ascontiguousarray(batch_embeddings)\n",
    "\n",
    "        # --- Online update ---\n",
    "        update_start = time.time()\n",
    "        okm.partial_fit(batch_embeddings)\n",
    "        update_end = time.time()\n",
    "        update_time = update_end - update_start\n",
    "\n",
    "        # --- Incrementally add batch to FAISS ---\n",
    "        if metric == \"cosine\":\n",
    "            faiss.normalize_L2(batch_embeddings)\n",
    "        faiss_index.add(batch_embeddings)\n",
    "\n",
    "        # Update index_to_chunk mapping\n",
    "        index_to_chunk.extend(df_chunks.iloc[init_size + start_idx : init_size + end_idx][\"context_id\"].tolist())\n",
    "\n",
    "        # --- Prepare queries for seen chunks ---\n",
    "        # Only keep queries whose context is in the FAISS index\n",
    "        df_queries_seen = df_queries[df_queries[\"context_id\"].isin(index_to_chunk)].reset_index(drop=True)\n",
    "\n",
    "        # --- Evaluate retrieval ---\n",
    "        retrieval_start = time.time()\n",
    "        metrics = evaluate_top_k_accuracy_faiss(\n",
    "            df_queries=df_queries_seen,\n",
    "            df_chunks=df_chunks,  # pass full df_chunks if needed\n",
    "            chunk_embeddings=None,  # embeddings not needed, FAISS handles search\n",
    "            faiss_index=faiss_index,\n",
    "            index_to_chunk=index_to_chunk,  # mapping for correct chunk IDs\n",
    "            top_k=top_k\n",
    "        )\n",
    "        retrieval_end = time.time()\n",
    "        retrieval_time = retrieval_end - retrieval_start\n",
    "\n",
    "        results.append({\n",
    "            \"batch\": batch_idx,\n",
    "            \"init_time\": init_time if batch_idx == 1 else 0,\n",
    "            \"update_time\": update_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"metrics\": metrics,\n",
    "            \"n_clusters\": len(okm.centroids)\n",
    "        })\n",
    "\n",
    "        print(f\"[Batch {batch_idx}] Seen chunks: {len(index_to_chunk)}, Doc acc: {metrics['doc_accuracy']:.4f}, Chunk acc: {metrics['chunk_accuracy']:.4f}, Clusters: {len(okm.centroids)}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_semantic_train = np.load(\"../data/tensors/squad_train_v2_semantic_chunking.npy\").astype(np.float32)\n",
    "df_semantic_train = pd.read_excel(\"../data/prepared/squad_train_v2_semantic_chunking.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"../data/prepared/squad_train_v2_queries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Using 50% of data (42003 samples) for initialization\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "results_df = online_kmeans_faiss_retrieval(\n",
    "    chunk_embeddings=X_semantic_train,\n",
    "    df_chunks=df_semantic_train,\n",
    "    df_queries=df_queries_train,\n",
    "    n_clusters=500,\n",
    "    max_clusters=2000,\n",
    "    batch_size=2000,\n",
    "    top_k=5,\n",
    "    init_fraction=0.5,\n",
    "    merge_threshold=0.08,\n",
    "    decay=1.0,\n",
    "    new_cluster_threshold=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"metrics\"].apply(lambda x: x['doc_accuracy']), marker=\"o\", label=\"Doc Accuracy\")\n",
    "plt.plot(results_df[\"batch\"], results_df[\"metrics\"].apply(lambda x: x['chunk_accuracy']), marker=\"s\", label=\"Chunk Accuracy\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"📊 Retrieval Accuracy per Batch (OnlineKMeans + FAISS)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Runtime ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"update_time\"], label=\"Update Time\", marker='o')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"retrieval_time\"], label=\"Retrieval Time\", marker='s')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"⚙️ Runtime per Batch (OnlineKMeans + FAISS)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Cluster count ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"n_clusters\"], marker='o', color='purple')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"# Clusters\")\n",
    "plt.title(\"📈 Cluster Count Evolution (OnlineKMeans)\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdk_szakdoga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
