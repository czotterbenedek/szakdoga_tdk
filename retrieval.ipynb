{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "This notebook contains the code for the retrival pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from OnlineKMeans import OnlineKMeans\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_centroids(chunk_embeddings, cluster_labels):\n",
    "    \"\"\"\n",
    "    Compute centroids once for all clusters.\n",
    "    Returns:\n",
    "        centroid_matrix: np.ndarray of shape (n_clusters, embedding_dim)\n",
    "        centroid_ids: list of cluster IDs\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cluster_centroids = {\n",
    "        cid: chunk_embeddings[cluster_labels == cid].mean(axis=0)\n",
    "        for cid in unique_clusters\n",
    "    }\n",
    "    centroid_matrix = np.vstack(list(cluster_centroids.values()))\n",
    "    centroid_ids = list(cluster_centroids.keys())\n",
    "    return centroid_matrix, centroid_ids\n",
    "\n",
    "\n",
    "def retrieve_top_chunks_by_cluster(\n",
    "    query_embedding,\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    cluster_labels,\n",
    "    centroid_matrix,\n",
    "    centroid_ids,\n",
    "    top_n_clusters=2,\n",
    "    top_k_total=5\n",
    "):\n",
    "    # --- Use precomputed centroids ---\n",
    "    cluster_sims = cosine_similarity([query_embedding], centroid_matrix)[0]\n",
    "    top_n_idx = cluster_sims.argsort()[::-1][:top_n_clusters]\n",
    "    selected_clusters = [centroid_ids[i] for i in top_n_idx]\n",
    "\n",
    "    # Collect all chunks from selected clusters\n",
    "    mask = np.isin(cluster_labels, selected_clusters)\n",
    "    selected_chunk_embeddings = chunk_embeddings[mask]\n",
    "    selected_df = df_chunks[mask].reset_index(drop=True)\n",
    "\n",
    "    # Compute similarity for all these chunks\n",
    "    sims = cosine_similarity([query_embedding], selected_chunk_embeddings)[0]\n",
    "\n",
    "    # Get top-K chunks overall\n",
    "    top_k_idx = sims.argsort()[::-1][:top_k_total]\n",
    "    results = []\n",
    "\n",
    "    for idx in top_k_idx:\n",
    "        results.append({\n",
    "            \"cluster\": cluster_labels[mask][idx],\n",
    "            \"context_id\": selected_df.iloc[idx][\"context_id\"],\n",
    "            \"chunk_id\": selected_df.iloc[idx][\"chunk_id\"],\n",
    "            \"title\": selected_df.iloc[idx][\"title\"],\n",
    "            \"chunk_embed_text\": selected_df.iloc[idx][\"chunk_embed_text\"],\n",
    "            \"chunk_start\": selected_df.iloc[idx][\"chunk_start\"],\n",
    "            \"chunk_end\": selected_df.iloc[idx][\"chunk_end\"],\n",
    "            \"similarity\": sims[idx]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"similarity\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_chunks_full(\n",
    "    query_embedding,\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    top_k_chunks=10\n",
    "):\n",
    "    sims = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    top_idx = sims.argsort()[::-1][:top_k_chunks]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        results.append({\n",
    "            \"context_id\": df_chunks.iloc[idx][\"context_id\"],\n",
    "            \"chunk_id\": df_chunks.iloc[idx][\"chunk_id\"],\n",
    "            \"title\": df_chunks.iloc[idx][\"title\"],\n",
    "            \"chunk_embed_text\": df_chunks.iloc[idx][\"chunk_embed_text\"],\n",
    "            \"chunk_start\": df_chunks.iloc[idx][\"chunk_start\"],\n",
    "            \"chunk_end\": df_chunks.iloc[idx][\"chunk_end\"],\n",
    "            \"similarity\": sims[idx]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"similarity\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- Answer Containment ----------\n",
    "# def is_answer_in_chunk(chunk_text, answer_text):\n",
    "#     return answer_text.lower().strip() in chunk_text.lower()\n",
    "\n",
    "# def is_answer_in_chunk(chunk_text, answer_text):\n",
    "#     # Normalize\n",
    "#     chunk_tokens = set(re.findall(r\"\\w+\", chunk_text.lower()))\n",
    "#     answer_tokens = set(re.findall(r\"\\w+\", answer_text.lower()))\n",
    "\n",
    "#     # Require that most/all answer tokens are present\n",
    "#     return len(answer_tokens & chunk_tokens) / max(1, len(answer_tokens)) >= 0.8\n",
    "\n",
    "\n",
    "# from rapidfuzz import fuzz\n",
    "\n",
    "# def is_answer_in_chunk(chunk_text, answer_text, threshold=80):\n",
    "#     score = fuzz.partial_ratio(answer_text.lower(), chunk_text.lower())\n",
    "#     return score >= threshold\n",
    "\n",
    "def is_answer_in_chunk(answer_start, chunk_start, chunk_length):\n",
    "    if answer_start is None or chunk_start is None or chunk_length is None:\n",
    "        return False\n",
    "    return chunk_start <= answer_start < (chunk_start + chunk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_for_query(results, query_row, similarity_threshold=0.75):\n",
    "    # --- Check similarity threshold ---\n",
    "    if results.empty or results[\"similarity\"].max() < similarity_threshold:\n",
    "        results_filtered = pd.DataFrame([])  # Treat as no answer\n",
    "    else:\n",
    "        results_filtered = results\n",
    "\n",
    "    # --- Document-level ---\n",
    "    answer_exists = pd.notna(query_row[\"answer_start\"])\n",
    "    found_doc_id = False if results_filtered.empty else any(\n",
    "        query_row[\"context_id\"] == doc_id for doc_id in results_filtered[\"context_id\"]\n",
    "    )\n",
    "    y_true_doc = 1 if answer_exists else 0\n",
    "    y_pred_doc = 1 if found_doc_id else 0\n",
    "\n",
    "    # --- Chunk-level ---\n",
    "    if results_filtered.empty:\n",
    "        found_chunk_context = False\n",
    "        good_chunks = 0\n",
    "    else:\n",
    "        correct_doc_chunks = results_filtered[results_filtered[\"context_id\"] == query_row[\"context_id\"]]\n",
    "        found_chunk_context = any(\n",
    "            is_answer_in_chunk(\n",
    "                query_row[\"answer_start\"],\n",
    "                chunk[\"chunk_start\"],\n",
    "                chunk[\"chunk_end\"] - chunk[\"chunk_start\"]\n",
    "            )\n",
    "            for _, chunk in results_filtered.iterrows()\n",
    "        )\n",
    "        good_chunks = len(correct_doc_chunks)\n",
    "\n",
    "    total_chunks = results_filtered.shape[0] if not results_filtered.empty else 1\n",
    "    chunk_ratio = good_chunks / total_chunks\n",
    "\n",
    "    y_true_chunk = 1 if answer_exists else 0\n",
    "    y_pred_chunk = 1 if found_chunk_context else 0\n",
    "\n",
    "    return y_true_doc, y_pred_doc, y_true_chunk, y_pred_chunk, chunk_ratio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_top_k_accuracy(\n",
    "#     df_queries,\n",
    "#     chunk_embeddings,\n",
    "#     df_chunks,\n",
    "#     cluster_labels,\n",
    "#     top_n_clusters=2,\n",
    "#     top_k_total=5\n",
    "# ):\n",
    "#     # ‚úÖ Compute centroids once\n",
    "#     centroid_matrix, centroid_ids = compute_cluster_centroids(chunk_embeddings, cluster_labels)\n",
    "\n",
    "#     y_true_doc = []\n",
    "#     y_pred_doc = []\n",
    "\n",
    "#     y_true_chunk = []\n",
    "#     y_pred_chunk = []\n",
    "\n",
    "#     chunk_ratios = []\n",
    "\n",
    "#     for i, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "#         query_emb = model.encode([row[\"question\"]])[0]\n",
    "#         results = retrieve_top_chunks_by_cluster(\n",
    "#             query_embedding=query_emb,\n",
    "#             chunk_embeddings=chunk_embeddings,\n",
    "#             df_chunks=df_chunks,\n",
    "#             cluster_labels=cluster_labels,\n",
    "#             centroid_matrix=centroid_matrix,\n",
    "#             centroid_ids=centroid_ids,\n",
    "#             top_n_clusters=top_n_clusters,\n",
    "#             top_k_total=top_k_total\n",
    "#         )\n",
    "\n",
    "#         # Document-level\n",
    "#         found_doc_id = any(row[\"context_id\"] == doc_id for doc_id in results[\"context_id\"])\n",
    "#         y_true_doc.append(1)\n",
    "#         y_pred_doc.append(1 if found_doc_id else 0)\n",
    "\n",
    "#         correct_doc_chunks = results[results[\"context_id\"] == row[\"context_id\"]]\n",
    "#         found_chunk_context = any(\n",
    "#             is_answer_in_chunk(\n",
    "#                 row[\"answer_start\"],\n",
    "#                 chunk[\"chunk_start\"],\n",
    "#                 chunk[\"chunk_end\"] - chunk[\"chunk_start\"]\n",
    "#             )\n",
    "#             for _, chunk in correct_doc_chunks.iterrows()\n",
    "#         )\n",
    "#         good_chunks = len(correct_doc_chunks)\n",
    "#         total_chunks = results.shape[0]\n",
    "#         ratio = good_chunks / total_chunks\n",
    "#         chunk_ratios.append(ratio)\n",
    "\n",
    "#         y_true_chunk.append(1)\n",
    "#         y_pred_chunk.append(1 if found_chunk_context else 0)\n",
    "\n",
    "#     # Compute metrics\n",
    "#     chunk_accuracy = sum(chunk_ratios) / len(chunk_ratios) if len(chunk_ratios) > 0 else 0\n",
    "#     metrics = {\n",
    "#         \"doc_accuracy\": sum(y_pred_doc) / len(y_pred_doc),\n",
    "#         \"chunk_accuracy\": sum(y_pred_chunk) / len(y_pred_chunk),\n",
    "#         \"doc_precision\": precision_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "#         \"doc_recall\": recall_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "#         \"doc_f1\": f1_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "#         \"chunk_precision\": precision_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "#         \"chunk_recall\": recall_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "#         \"chunk_f1\": f1_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "#         \"correct_chunk_accuracy\": chunk_accuracy\n",
    "#     }\n",
    "\n",
    "#     return metrics\n",
    "def evaluate_top_k_accuracy(\n",
    "    df_queries,\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    cluster_labels,\n",
    "    top_n_clusters=2,\n",
    "    top_k_total=5,\n",
    "    similarity_threshold=0.6\n",
    "):\n",
    "    # Compute centroids once\n",
    "    centroid_matrix, centroid_ids = compute_cluster_centroids(chunk_embeddings, cluster_labels)\n",
    "\n",
    "    y_true_doc = []\n",
    "    y_pred_doc = []\n",
    "\n",
    "    y_true_chunk = []\n",
    "    y_pred_chunk = []\n",
    "\n",
    "    chunk_ratios = []\n",
    "\n",
    "    for _, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "        query_emb = model.encode([row[\"question\"]])[0]\n",
    "        results = retrieve_top_chunks_by_cluster(\n",
    "            query_embedding=query_emb,\n",
    "            chunk_embeddings=chunk_embeddings,\n",
    "            df_chunks=df_chunks,\n",
    "            cluster_labels=cluster_labels,\n",
    "            centroid_matrix=centroid_matrix,\n",
    "            centroid_ids=centroid_ids,\n",
    "            top_n_clusters=top_n_clusters,\n",
    "            top_k_total=top_k_total\n",
    "        )\n",
    "\n",
    "        ytd, ypd, ytc, ypc, cr = compute_metrics_for_query(results, row, similarity_threshold)\n",
    "        y_true_doc.append(ytd)\n",
    "        y_pred_doc.append(ypd)\n",
    "        y_true_chunk.append(ytc)\n",
    "        y_pred_chunk.append(ypc)\n",
    "        chunk_ratios.append(cr)\n",
    "\n",
    "    # Convert to arrays\n",
    "    y_true_doc_arr = np.array(y_true_doc)\n",
    "    y_pred_doc_arr = np.array(y_pred_doc)\n",
    "    y_true_chunk_arr = np.array(y_true_chunk)\n",
    "    y_pred_chunk_arr = np.array(y_pred_chunk)\n",
    "\n",
    "    # Compute metrics\n",
    "    chunk_accuracy = sum(chunk_ratios) / len(chunk_ratios) if len(chunk_ratios) > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        \"doc_accuracy\": (y_pred_doc_arr == y_true_doc_arr).mean(),\n",
    "        \"chunk_accuracy\": (y_pred_chunk_arr == y_true_chunk_arr).mean(),\n",
    "        \"doc_precision\": precision_score(y_true_doc_arr, y_pred_doc_arr, zero_division=0),\n",
    "        \"doc_recall\": recall_score(y_true_doc_arr, y_pred_doc_arr, zero_division=0),\n",
    "        \"doc_f1\": f1_score(y_true_doc_arr, y_pred_doc_arr, zero_division=0),\n",
    "        \"chunk_precision\": precision_score(y_true_chunk_arr, y_pred_chunk_arr, zero_division=0),\n",
    "        \"chunk_recall\": recall_score(y_true_chunk_arr, y_pred_chunk_arr, zero_division=0),\n",
    "        \"chunk_f1\": f1_score(y_true_chunk_arr, y_pred_chunk_arr, zero_division=0),\n",
    "        \"correct_chunk_accuracy\": chunk_accuracy,\n",
    "        # True/False Positives/Negatives\n",
    "        \"doc_true_positives\": np.sum((y_pred_doc_arr == 1) & (y_true_doc_arr == 1)),\n",
    "        \"doc_true_negatives\": np.sum((y_pred_doc_arr == 0) & (y_true_doc_arr == 0)),\n",
    "        \"doc_false_positives\": np.sum((y_pred_doc_arr == 1) & (y_true_doc_arr == 0)),\n",
    "        \"doc_false_negatives\": np.sum((y_pred_doc_arr == 0) & (y_true_doc_arr == 1)),\n",
    "        \"chunk_true_positives\": np.sum((y_pred_chunk_arr == 1) & (y_true_chunk_arr == 1)),\n",
    "        \"chunk_true_negatives\": np.sum((y_pred_chunk_arr == 0) & (y_true_chunk_arr == 0)),\n",
    "        \"chunk_false_positives\": np.sum((y_pred_chunk_arr == 1) & (y_true_chunk_arr == 0)),\n",
    "        \"chunk_false_negatives\": np.sum((y_pred_chunk_arr == 0) & (y_true_chunk_arr == 1)),\n",
    "    }\n",
    "\n",
    "    return metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def evaluate_top_k_accuracy_full(df_queries, chunk_embeddings, df_chunks, top_k_chunks=5, similarity_threshold=0.6):\n",
    "    # y_true_doc = []\n",
    "    # y_pred_doc = []\n",
    "\n",
    "    # y_true_chunk = []\n",
    "    # y_pred_chunk = []\n",
    "\n",
    "    # chunk_ratios = []\n",
    "\n",
    "    # for i, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "    #     query_emb = model.encode([row[\"question\"]])[0]\n",
    "    #     results = retrieve_top_chunks_full(\n",
    "    #         query_embedding=query_emb,\n",
    "    #         chunk_embeddings=chunk_embeddings,\n",
    "    #         df_chunks=df_chunks,\n",
    "    #         top_k_chunks=top_k_chunks\n",
    "    #     )\n",
    "\n",
    "    #     # Document-level\n",
    "    #     found_doc_id = any(row[\"context_id\"] == doc_id for doc_id in results[\"context_id\"])\n",
    "    #     y_true_doc.append(1)\n",
    "    #     y_pred_doc.append(1 if found_doc_id else 0)\n",
    "\n",
    "    #     correct_doc_chunks = results[results[\"context_id\"] == row[\"context_id\"]]\n",
    "    #     found_chunk_context = any(\n",
    "    #         is_answer_in_chunk(\n",
    "    #             row[\"answer_start\"],\n",
    "    #             chunk[\"chunk_start\"],\n",
    "    #             chunk[\"chunk_end\"] - chunk[\"chunk_start\"]\n",
    "    #         )\n",
    "    #         for _, chunk in correct_doc_chunks.iterrows()\n",
    "    #     )\n",
    "    #     good_chunks = len(correct_doc_chunks)\n",
    "    #     total_chunks = results.shape[0]\n",
    "    #     ratio = good_chunks / total_chunks\n",
    "    #     chunk_ratios.append(ratio)\n",
    "\n",
    "    #     y_true_chunk.append(1)\n",
    "    #     y_pred_chunk.append(1 if found_chunk_context else 0)\n",
    "\n",
    "    # # Compute metrics\n",
    "    # chunk_accuracy = sum(chunk_ratios) / len(chunk_ratios) if len(chunk_ratios) > 0 else 0\n",
    "    # metrics = {\n",
    "    #     \"doc_accuracy\": sum(y_pred_doc) / len(y_pred_doc),\n",
    "    #     \"chunk_accuracy\": sum(y_pred_chunk) / len(y_pred_chunk),\n",
    "    #     \"doc_precision\": precision_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "    #     \"doc_recall\": recall_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "    #     \"doc_f1\": f1_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "    #     \"chunk_precision\": precision_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "    #     \"chunk_recall\": recall_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "    #     \"chunk_f1\": f1_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "    #     \"correct_chunk_accuracy\": chunk_accuracy\n",
    "    # }\n",
    "\n",
    "    # return metrics\n",
    "def evaluate_top_k_accuracy_full(df_queries, chunk_embeddings, df_chunks, top_k_chunks=5, similarity_threshold=0.7):\n",
    "    y_true_doc = []\n",
    "    y_pred_doc = []\n",
    "\n",
    "    y_true_chunk = []\n",
    "    y_pred_chunk = []\n",
    "\n",
    "    chunk_ratios = []\n",
    "\n",
    "    for _, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "        query_emb = model.encode([row[\"question\"]])[0]\n",
    "        results = retrieve_top_chunks_full(\n",
    "            query_embedding=query_emb,\n",
    "            chunk_embeddings=chunk_embeddings,\n",
    "            df_chunks=df_chunks,\n",
    "            top_k_chunks=top_k_chunks\n",
    "        )\n",
    "\n",
    "        ytd, ypd, ytc, ypc, cr = compute_metrics_for_query(results, row, similarity_threshold)\n",
    "        y_true_doc.append(ytd)\n",
    "        y_pred_doc.append(ypd)\n",
    "        y_true_chunk.append(ytc)\n",
    "        y_pred_chunk.append(ypc)\n",
    "        chunk_ratios.append(cr)\n",
    "\n",
    "    # Convert to arrays\n",
    "    y_true_doc_arr = np.array(y_true_doc)\n",
    "    y_pred_doc_arr = np.array(y_pred_doc)\n",
    "    y_true_chunk_arr = np.array(y_true_chunk)\n",
    "    y_pred_chunk_arr = np.array(y_pred_chunk)\n",
    "\n",
    "    # Compute metrics\n",
    "    chunk_accuracy = sum(chunk_ratios) / len(chunk_ratios) if len(chunk_ratios) > 0 else 0\n",
    "\n",
    "    metrics = {\n",
    "        \"doc_accuracy\": (y_pred_doc_arr == y_true_doc_arr).mean(),\n",
    "        \"chunk_accuracy\": (y_pred_chunk_arr == y_true_chunk_arr).mean(),\n",
    "        \"doc_precision\": precision_score(y_true_doc_arr, y_pred_doc_arr, zero_division=0),\n",
    "        \"doc_recall\": recall_score(y_true_doc_arr, y_pred_doc_arr, zero_division=0),\n",
    "        \"doc_f1\": f1_score(y_true_doc_arr, y_pred_doc_arr, zero_division=0),\n",
    "        \"chunk_precision\": precision_score(y_true_chunk_arr, y_pred_chunk_arr, zero_division=0),\n",
    "        \"chunk_recall\": recall_score(y_true_chunk_arr, y_pred_chunk_arr, zero_division=0),\n",
    "        \"chunk_f1\": f1_score(y_true_chunk_arr, y_pred_chunk_arr, zero_division=0),\n",
    "        \"correct_chunk_accuracy\": chunk_accuracy,\n",
    "        # True/False Positives/Negatives\n",
    "        \"doc_true_positives\": np.sum((y_pred_doc_arr == 1) & (y_true_doc_arr == 1)),\n",
    "        \"doc_true_negatives\": np.sum((y_pred_doc_arr == 0) & (y_true_doc_arr == 0)),\n",
    "        \"doc_false_positives\": np.sum((y_pred_doc_arr == 1) & (y_true_doc_arr == 0)),\n",
    "        \"doc_false_negatives\": np.sum((y_pred_doc_arr == 0) & (y_true_doc_arr == 1)),\n",
    "        \"chunk_true_positives\": np.sum((y_pred_chunk_arr == 1) & (y_true_chunk_arr == 1)),\n",
    "        \"chunk_true_negatives\": np.sum((y_pred_chunk_arr == 0) & (y_true_chunk_arr == 0)),\n",
    "        \"chunk_false_positives\": np.sum((y_pred_chunk_arr == 1) & (y_true_chunk_arr == 0)),\n",
    "        \"chunk_false_negatives\": np.sum((y_pred_chunk_arr == 0) & (y_true_chunk_arr == 1)),\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatchkmeans_retrieval_evaluation(\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    df_queries,\n",
    "    n_clusters=20,\n",
    "    batch_size=500,\n",
    "    top_k_total=5,\n",
    "    init_fraction=0.1\n",
    "):\n",
    "    n_samples = chunk_embeddings.shape[0]\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # --- Inicializ√°l√≥ klaszterez√©s ---\n",
    "    init_start = time.time()\n",
    "    init_size = max(1, int(n_samples * init_fraction))\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=batch_size)\n",
    "    kmeans.partial_fit(chunk_embeddings[:init_size])\n",
    "    init_end = time.time()\n",
    "    init_time = init_end - init_start\n",
    "\n",
    "    print(\"Start batch processing...\")\n",
    "    for batch_idx in tqdm(range(1, n_batches + 1)):\n",
    "        start_idx = (batch_idx - 1) * batch_size\n",
    "        end_idx = min(batch_idx * batch_size, n_samples)\n",
    "        X_batch = chunk_embeddings[start_idx:end_idx]\n",
    "\n",
    "        # --- Online update ---\n",
    "        update_start = time.time()\n",
    "        kmeans.partial_fit(X_batch)\n",
    "        update_end = time.time()\n",
    "        update_time = update_end - update_start\n",
    "\n",
    "        # --- Klaszterc√≠mk√©k friss√≠t√©se ---\n",
    "        labels = kmeans.predict(chunk_embeddings)\n",
    "\n",
    "        # --- Retrieval + pontoss√°g ---\n",
    "        retrieval_start = time.time()\n",
    "        metrics = evaluate_top_k_accuracy(\n",
    "            df_queries=df_queries,\n",
    "            chunk_embeddings=chunk_embeddings,\n",
    "            df_chunks=df_chunks,\n",
    "            cluster_labels=labels,\n",
    "            top_n_clusters=5,\n",
    "            top_k_total=top_k_total\n",
    "        )\n",
    "        retrieval_end = time.time()\n",
    "        retrieval_time = retrieval_end - retrieval_start\n",
    "\n",
    "        results.append({\n",
    "            \"batch\": batch_idx,\n",
    "            \"init_time\": init_time if batch_idx == 1 else 0,\n",
    "            \"update_time\": update_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"metrics\": metrics,\n",
    "        })\n",
    "        print(f\"[Batch {batch_idx}/{n_batches}] Doc acc: {metrics['doc_accuracy']:.4f}, Chunk acc: {metrics['chunk_accuracy']:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_kmeans_retrieval_evaluation(\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    df_queries,\n",
    "    n_clusters=20,\n",
    "    batch_size=500,\n",
    "    top_k_total=5,\n",
    "    init_fraction=0.5,  # fraction of data used for initialization\n",
    "    max_clusters=None,\n",
    "    metric=\"cosine\",\n",
    "    new_cluster_threshold=None,\n",
    "    merge_threshold=None,\n",
    "    decay=None\n",
    "):\n",
    "    \"\"\"\n",
    "    OnlineKMeans clustering + retrieval evaluation on growing dataset.\n",
    "    Only evaluates on the chunks that have been clustered so far.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = chunk_embeddings.shape[0]\n",
    "    init_size = int(n_samples * init_fraction)\n",
    "    remaining_size = n_samples - init_size\n",
    "\n",
    "    # --- Step 1: Initialization ---\n",
    "    print(f\"üîß Using {init_fraction*100:.0f}% of data ({init_size} samples) for initialization\")\n",
    "    init_start = time.time()\n",
    "    okm = OnlineKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        max_clusters=max_clusters,\n",
    "        metric=metric,\n",
    "        new_cluster_threshold=new_cluster_threshold,\n",
    "        merge_threshold=merge_threshold,\n",
    "        random_state=42,\n",
    "        decay=decay\n",
    "    )\n",
    "    okm.partial_fit(chunk_embeddings[:init_size])\n",
    "    init_end = time.time()\n",
    "    init_time = init_end - init_start\n",
    "    print(f\"‚úÖ Initialization done in {init_time:.4f} s\")\n",
    "\n",
    "    # --- Step 2: Online updates on the remaining data ---\n",
    "    results = []\n",
    "    for batch_idx in tqdm(range(1, int(np.ceil(remaining_size / batch_size)) + 1)):\n",
    "        start_idx = (batch_idx - 1) * batch_size\n",
    "        end_idx = min(batch_idx * batch_size, remaining_size)\n",
    "        batch_embeddings = chunk_embeddings[init_size + start_idx : init_size + end_idx]\n",
    "\n",
    "        # --- Online update ---\n",
    "        update_start = time.time()\n",
    "        okm.partial_fit(batch_embeddings)\n",
    "        update_end = time.time()\n",
    "        update_time = update_end - update_start\n",
    "\n",
    "        # --- Only evaluate on seen data so far ---\n",
    "        seen_end_idx = init_size + end_idx\n",
    "        seen_embeddings = chunk_embeddings[:seen_end_idx]\n",
    "        seen_df_chunks = df_chunks.iloc[:seen_end_idx].reset_index(drop=True)\n",
    "\n",
    "        # --- Predict cluster labels for seen data ---\n",
    "        labels_seen = okm.predict(seen_embeddings)\n",
    "\n",
    "        # --- Filter queries to only those with seen context_ids ---\n",
    "        # df_queries_seen = df_queries[df_queries[\"context_id\"].isin(seen_df_chunks[\"context_id\"].unique())].reset_index(drop=True)\n",
    "        # print(f\"df_queries_seen: {df_queries_seen.shape[0]}, seen_df_chunks: {seen_df_chunks.shape[0]}\")\n",
    "\n",
    "        # --- Retrieval accuracy ---\n",
    "        retrieval_start = time.time()\n",
    "        metrics = evaluate_top_k_accuracy(\n",
    "            df_queries=df_queries,\n",
    "            chunk_embeddings=seen_embeddings,\n",
    "            df_chunks=seen_df_chunks,\n",
    "            cluster_labels=labels_seen,\n",
    "            top_n_clusters=5,\n",
    "            top_k_total=top_k_total\n",
    "        )\n",
    "        retrieval_end = time.time()\n",
    "        retrieval_time = retrieval_end - retrieval_start\n",
    "\n",
    "        results.append({\n",
    "            \"batch\": batch_idx,\n",
    "            \"init_time\": init_time if batch_idx == 1 else 0,\n",
    "            \"update_time\": update_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"metrics\": metrics,\n",
    "            \"n_clusters\": len(okm.centroids)\n",
    "        })\n",
    "\n",
    "        results_df = pd.DataFrame(results)\n",
    "        results_df.to_excel(\"./onlinekmeans_test.xlsx\")\n",
    "\n",
    "        print(f\"[Batch {batch_idx}] Seen chunks: {seen_end_idx}, Doc acc: {metrics['doc_accuracy']:.4f}, Chunk acc: {metrics['chunk_accuracy']:.4f}, Clusters: {len(okm.centroids)}\")\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"./data/labelled/squad_train_v2_semantic_chunking_clustered.xlsx\")\n",
    "df_val = pd.read_excel(\"./data/labelled/squad_val_v2_semantic_chunking_clustered.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"./data/prepared/squad_train_v2_queries.xlsx\")\n",
    "df_queries_train = df_queries_train[df_queries_train[\"context_id\"].isin(df_train[\"context_id\"].unique())].reset_index(drop=True)\n",
    "\n",
    "X_train = np.load(\"./data/labelled/squad_train_v2_semantic_chunking_clustered.npy\")\n",
    "X_val = np.load(\"./data/labelled/squad_val_v2_semantic_chunking_clustered.npy\")\n",
    "df_queries_val = pd.read_excel(\"./data/prepared/squad_val_v2_queries.xlsx\")\n",
    "\n",
    "labels_train = df_train[\"cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(84007, 10)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(87599, 4)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'What is in front of the Notre Dame Main Building?'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries_train.loc[1, \"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = df_queries_train.loc[50000, \"question\"]\n",
    "query_emb = model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "top_chunks_cluster = retrieve_top_chunks_by_cluster(\n",
    "    query_embedding=query_emb,\n",
    "    chunk_embeddings=X_train,\n",
    "    df_chunks=df_train,\n",
    "    cluster_labels=labels_train,\n",
    "    top_n_clusters=3,\n",
    "    top_k_total=3\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Runtime: {elapsed_time:.5f} seconds\")\n",
    "print(\"Cluster-based retrieval:\")\n",
    "print(top_chunks_cluster['chunk_embed_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.78553945 0.710261   0.69749516 0.67169    0.6706306  0.6444305\n",
      " 0.6248773  0.6170142  0.6095041  0.5957897 ]\n",
      "Runtime: 0.08458 seconds\n",
      "Full retrieval:\n",
      "['It was accepted as a territory of Australia, separate from any state, by the Norfolk Island Act 1913 (Cth), passed under the territories power (Constitution section 122) and made effective in 1914. In 1976 the High Court of Australia held unanimously that Norfolk Island is a part of the Commonwealth.', 'The Norfolk Island Act 1979, passed by the Parliament of Australia in 1979, is the Act under which the island was governed until the passing of the Norfolk Island Legislation Amendment Act 2015.', 'After the creation of the Commonwealth of Australia in 1901, Norfolk Island was placed under the authority of the new Commonwealth government to be administered as an external territory.', 'Norfolk Island was colonised by East Polynesians but was long unpeopled when it was settled by Great Britain as part of its settlement of Australia from 1788.', 'In 1913, the UK handed Norfolk over to Australia to administer as an external territory.', 'Norfolk Island is located in the South Pacific Ocean, east of the Australian mainland. Norfolk Island is the main island of the island group the territory encompasses and is located at 29¬∞02‚Ä≤S 167¬∞57‚Ä≤E\\ufeff / \\ufeff29.033¬∞S 167.950¬∞E\\ufeff / -29.033; 167.950.', 'The Norfolk Island Legislative Assembly decided to hold a referendum on the proposal. On 8 May 2015, voters were asked if Norfolk Islanders should freely determine their political status and their economic, social and cultural development, and to \"be consulted at referendum or plebiscite on the future model of governance for Norfolk Island before such changes are acted upon by the Australian parliament\".', \"Norfolk Island (i/Ààn…îÀêrf…ôk Ààa…™l…ônd/; Norfuk: Norf'k Ailen) is a small island in the Pacific Ocean located between Australia, New Zealand and New Caledonia, 1,412 kilometres (877 mi) directly east of mainland Australia's Evans Head, and about 900 kilometres (560 mi) from Lord Howe Island.\", 'Norfolk Island was originally a colony acquired by settlement but was never within the British Settlements Act.', 'The Norfolk Island Chief Minister, Lisle Snell, said that \"the referendum results blow a hole in Canberra\\'s assertion that the reforms introduced before the Australian Parliament that propose abolishing the Legislative Assembly and Norfolk Island Parliament were overwhelmingly supported by the people of Norfolk Island\".']\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "top_chunks_full = retrieve_top_chunks_full(\n",
    "    query_embedding=query_emb,\n",
    "    chunk_embeddings=X_train,\n",
    "    df_chunks=df_train,\n",
    "    top_k_chunks=10\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Runtime: {elapsed_time:.5f} seconds\")\n",
    "print(\"Full retrieval:\")\n",
    "print(top_chunks_full['chunk_embed_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for retrival with cluster centroids vs full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_semantic_train = np.load(\"./data/tensors/squad_train_v2_semantic_chunking_l2.npy\")\n",
    "df_semantic_train = pd.read_excel(\"./data/labelled/squad_train_v2_semantic_chunking_clustered_kmeans180.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"./data/prepared/squad_train_v2_queries.xlsx\")\n",
    "\n",
    "labels_train = df_semantic_train[\"cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-3 chunks in Top-5 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87599/87599 [16:20<00:00, 89.37it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-3 chunks in Top-10 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87599/87599 [24:30<00:00, 59.58it/s]    \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-3 chunks in Top-20 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 87599/87599 [31:50<00:00, 45.84it/s]    \n",
      " 22%|‚ñà‚ñà‚ñè       | 19313/87599 [13:25<47:29, 23.96it/s]  \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[66]\u001b[39m\u001b[32m, line 31\u001b[39m\n\u001b[32m     28\u001b[39m     results_df_centroid.to_excel(\u001b[33m\"\u001b[39m\u001b[33m./centroid_test.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     30\u001b[39m start_full = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m31\u001b[39m full_metrics = \u001b[43mevaluate_top_k_accuracy_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_queries_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_semantic_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_semantic_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m end_full = time.time()\n\u001b[32m     34\u001b[39m results_full.append({\n\u001b[32m     35\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m: top_k,\n\u001b[32m     36\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfull_metrics\u001b[39m\u001b[33m\"\u001b[39m: full_metrics,\n\u001b[32m     37\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfull_time\u001b[39m\u001b[33m\"\u001b[39m: end_full - start_full\n\u001b[32m     38\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[59]\u001b[39m\u001b[32m, line 67\u001b[39m, in \u001b[36mevaluate_top_k_accuracy_full\u001b[39m\u001b[34m(df_queries, chunk_embeddings, df_chunks, top_k_chunks, similarity_threshold)\u001b[39m\n\u001b[32m     65\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m _, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df_queries.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(df_queries)):\n\u001b[32m     66\u001b[39m     query_emb = model.encode([row[\u001b[33m\"\u001b[39m\u001b[33mquestion\u001b[39m\u001b[33m\"\u001b[39m]])[\u001b[32m0\u001b[39m]\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m     results = \u001b[43mretrieve_top_chunks_full\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     68\u001b[39m \u001b[43m        \u001b[49m\u001b[43mquery_embedding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mquery_emb\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     69\u001b[39m \u001b[43m        \u001b[49m\u001b[43mchunk_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mchunk_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     70\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdf_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdf_chunks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     71\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtop_k_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k_chunks\u001b[49m\n\u001b[32m     72\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     74\u001b[39m     ytd, ypd, ytc, ypc, cr = compute_metrics_for_query(results, row, similarity_threshold)\n\u001b[32m     75\u001b[39m     y_true_doc.append(ytd)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[55]\u001b[39m\u001b[32m, line 8\u001b[39m, in \u001b[36mretrieve_top_chunks_full\u001b[39m\u001b[34m(query_embedding, chunk_embeddings, df_chunks, top_k_chunks)\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mretrieve_top_chunks_full\u001b[39m(\n\u001b[32m      2\u001b[39m     query_embedding,\n\u001b[32m      3\u001b[39m     chunk_embeddings,\n\u001b[32m      4\u001b[39m     df_chunks,\n\u001b[32m      5\u001b[39m     top_k_chunks=\u001b[32m10\u001b[39m\n\u001b[32m      6\u001b[39m ):\n\u001b[32m      7\u001b[39m     sims = cosine_similarity([query_embedding], chunk_embeddings)[\u001b[32m0\u001b[39m]\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     top_idx = \u001b[43msims\u001b[49m\u001b[43m.\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[::-\u001b[32m1\u001b[39m][:top_k_chunks]\n\u001b[32m     10\u001b[39m     results = []\n\u001b[32m     11\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m top_idx:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Benchmark\n",
    "# top_ks = [3, 5, 12, 25]\n",
    "top_ks = [3]\n",
    "top_n_clusters = [5, 10, 20]\n",
    "\n",
    "# top_ks = [5, 10, 20]\n",
    "# top_n_clusters = [5, 10, 20]\n",
    " \n",
    "\n",
    "results_centroid = []\n",
    "results_full = []\n",
    "\n",
    "for top_k in top_ks:\n",
    "    for top_n_cluster in top_n_clusters:\n",
    "\n",
    "        print(f\"Evaluating: Top-{top_k} chunks in Top-{top_n_cluster} clusters\")\n",
    "        start_centroid = time.time()\n",
    "        centroid_metrics = evaluate_top_k_accuracy(df_queries_train, X_semantic_train, df_semantic_train, labels_train, top_n_clusters=top_n_cluster, top_k_total=top_k)\n",
    "        end_centroid = time.time()\n",
    "\n",
    "        results_centroid.append({\n",
    "            \"top_k\": top_k,\n",
    "            \"top_n_clusters\": top_n_cluster,\n",
    "            \"centroid_metrics\": centroid_metrics,\n",
    "            \"centroid_time\": end_centroid - start_centroid\n",
    "        }) \n",
    "        results_df_centroid = pd.DataFrame(results_centroid)\n",
    "        results_df_centroid.to_excel(\"./centroid_test.xlsx\")\n",
    "\n",
    "    start_full = time.time()\n",
    "    full_metrics = evaluate_top_k_accuracy_full(df_queries_train, X_semantic_train, df_semantic_train, top_k_chunks=top_k)\n",
    "    end_full = time.time()\n",
    "\n",
    "    results_full.append({\n",
    "        \"top_k\": top_k,\n",
    "        \"full_metrics\": full_metrics,\n",
    "        \"full_time\": end_full - start_full\n",
    "    })\n",
    "    results_df_full = pd.DataFrame(results_full)\n",
    "    results_df_full.to_excel(\"./full_test.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster-based:\n",
      "Doc accuracy: 0.7683, Chunk accuracy: 0.6987, Relative chunk/doc accuracy: 0.9095\n",
      "Full retrieval:\n",
      "Doc accuracy: 0.8522, Chunk accuracy: 0.7841, Relative chunk/doc accuracy: 0.9200\n"
     ]
    }
   ],
   "source": [
    "relative_doc_acc_centroid = centroid_metrics['chunk_accuracy'] / centroid_metrics['doc_accuracy']\n",
    "relative_doc_acc_full = full_metrics['chunk_accuracy'] / full_metrics['doc_accuracy']\n",
    "\n",
    "print(f\"Cluster-based:\")\n",
    "print(f\"Doc accuracy: {centroid_metrics['doc_accuracy']:.4f}, Chunk accuracy: {centroid_metrics['chunk_accuracy']:.4f}, Relative chunk/doc accuracy: {relative_doc_acc_centroid:.4f}\")\n",
    "print(f\"Full retrieval:\")\n",
    "print(f\"Doc accuracy: {full_metrics['doc_accuracy']:.4f}, Chunk accuracy: {full_metrics['chunk_accuracy']:.4f}, Relative chunk/doc accuracy: {relative_doc_acc_full:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate retrieval with MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_semantic_train = np.load(\"./data/tensors/squad_train_v2_semantic_chunking.npy\")\n",
    "df_semantic_train = pd.read_excel(\"./data/prepared/squad_train_v2_semantic_chunking.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"./data/prepared/squad_train_v2_queries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Futtat√°s ---\n",
    "results_df = minibatchkmeans_retrieval_evaluation(\n",
    "    chunk_embeddings=X_semantic_train,\n",
    "    df_chunks=df_semantic_train,\n",
    "    df_queries=df_queries_train,\n",
    "    n_clusters=160,\n",
    "    batch_size=1000,\n",
    "    top_k_total=5,\n",
    "    init_fraction=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotol√°s: Pontoss√°g ---\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"doc_accuracy\"], label=\"Doc Accuracy\", marker='o')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"chunk_accuracy\"], label=\"Chunk Accuracy\", marker='s')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Retrieval pontoss√°g batchenk√©nt (Online KMeans)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plotol√°s: Fut√°sid≈ëk ---\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"init_time\"], label=\"Init time\", marker='o')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"update_time\"], label=\"Update time\", marker='s')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"retrieval_time\"], label=\"Retrieval time\", marker='^')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Fut√°sid≈ëk batchenk√©nt (Online KMeans)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate retrieval with online clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_semantic_train = np.load(\"./data/tensors/squad_train_v2_semantic_chunking_l2.npy\")\n",
    "df_semantic_train = pd.read_excel(\"./data/prepared/squad_train_v2_semantic_chunking.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"./data/prepared/squad_train_v2_queries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_queries_train.loc[25000:50000, 'answer_start'] = None\n",
    "\n",
    "np.random.seed(42)\n",
    "n_rows = len(df_queries_train)\n",
    "random_indices = np.random.choice(n_rows, size=int(0.15 * n_rows), replace=False)\n",
    "df_queries_train.loc[random_indices, 'answer_start'] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df = online_kmeans_retrieval_evaluation(\n",
    "    chunk_embeddings=X_semantic_train,\n",
    "    df_chunks=df_semantic_train,\n",
    "    df_queries=df_queries_train,\n",
    "    n_clusters=500,\n",
    "    max_clusters=2000,\n",
    "    batch_size=2000,\n",
    "    top_k_total=5,\n",
    "    metric=\"cosine\",\n",
    "    init_fraction=0.5,\n",
    "    merge_threshold=0.08,    \n",
    "    decay=1.0,\n",
    "    new_cluster_threshold=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tdk_szakdoga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
