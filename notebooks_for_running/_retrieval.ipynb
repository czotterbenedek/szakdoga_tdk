{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Retrieval\n",
    "\n",
    "This notebook contains the code for the retrival pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import warnings\n",
    "import re\n",
    "\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "from OnlineKMeans import OnlineKMeans\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classes and functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cluster_centroids(chunk_embeddings, cluster_labels):\n",
    "    \"\"\"\n",
    "    Compute centroids once for all clusters.\n",
    "    Returns:\n",
    "        centroid_matrix: np.ndarray of shape (n_clusters, embedding_dim)\n",
    "        centroid_ids: list of cluster IDs\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(cluster_labels)\n",
    "    cluster_centroids = {\n",
    "        cid: chunk_embeddings[cluster_labels == cid].mean(axis=0)\n",
    "        for cid in unique_clusters\n",
    "    }\n",
    "    centroid_matrix = np.vstack(list(cluster_centroids.values()))\n",
    "    centroid_ids = list(cluster_centroids.keys())\n",
    "    return centroid_matrix, centroid_ids\n",
    "\n",
    "\n",
    "def retrieve_top_chunks_by_cluster(\n",
    "    query_embedding,\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    cluster_labels,\n",
    "    centroid_matrix,\n",
    "    centroid_ids,\n",
    "    top_n_clusters=2,\n",
    "    top_k_total=5\n",
    "):\n",
    "    # --- Use precomputed centroids ---\n",
    "    cluster_sims = cosine_similarity([query_embedding], centroid_matrix)[0]\n",
    "    top_n_idx = cluster_sims.argsort()[::-1][:top_n_clusters]\n",
    "    selected_clusters = [centroid_ids[i] for i in top_n_idx]\n",
    "\n",
    "    # Collect all chunks from selected clusters\n",
    "    mask = np.isin(cluster_labels, selected_clusters)\n",
    "    selected_chunk_embeddings = chunk_embeddings[mask]\n",
    "    selected_df = df_chunks[mask].reset_index(drop=True)\n",
    "\n",
    "    # Compute similarity for all these chunks\n",
    "    sims = cosine_similarity([query_embedding], selected_chunk_embeddings)[0]\n",
    "\n",
    "    # Get top-K chunks overall\n",
    "    top_k_idx = sims.argsort()[::-1][:top_k_total]\n",
    "    results = []\n",
    "\n",
    "    for idx in top_k_idx:\n",
    "        results.append({\n",
    "            \"cluster\": cluster_labels[mask][idx],\n",
    "            \"context_id\": selected_df.iloc[idx][\"context_id\"],\n",
    "            \"chunk_id\": selected_df.iloc[idx][\"chunk_id\"],\n",
    "            \"title\": selected_df.iloc[idx][\"title\"],\n",
    "            \"chunk_embed_text\": selected_df.iloc[idx][\"chunk_embed_text\"],\n",
    "            \"chunk_start\": selected_df.iloc[idx][\"chunk_start\"],\n",
    "            \"chunk_end\": selected_df.iloc[idx][\"chunk_end\"],\n",
    "            \"similarity\": sims[idx]\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(results).sort_values(\"similarity\", ascending=False).reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_top_chunks_full(\n",
    "    query_embedding,\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    top_k_chunks=10\n",
    "):\n",
    "    sims = cosine_similarity([query_embedding], chunk_embeddings)[0]\n",
    "    top_idx = sims.argsort()[::-1][:top_k_chunks]\n",
    "    \n",
    "    results = []\n",
    "    for idx in top_idx:\n",
    "        results.append({\n",
    "            \"context_id\": df_chunks.iloc[idx][\"context_id\"],\n",
    "            \"chunk_id\": df_chunks.iloc[idx][\"chunk_id\"],\n",
    "            \"title\": df_chunks.iloc[idx][\"title\"],\n",
    "            \"chunk_embed_text\": df_chunks.iloc[idx][\"chunk_embed_text\"],\n",
    "            \"chunk_start\": df_chunks.iloc[idx][\"chunk_start\"],\n",
    "            \"chunk_end\": df_chunks.iloc[idx][\"chunk_end\"],\n",
    "            \"similarity\": sims[idx]\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(results).sort_values(\"similarity\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # ---------- Answer Containment ----------\n",
    "# def is_answer_in_chunk(chunk_text, answer_text):\n",
    "#     return answer_text.lower().strip() in chunk_text.lower()\n",
    "\n",
    "# def is_answer_in_chunk(chunk_text, answer_text):\n",
    "#     # Normalize\n",
    "#     chunk_tokens = set(re.findall(r\"\\w+\", chunk_text.lower()))\n",
    "#     answer_tokens = set(re.findall(r\"\\w+\", answer_text.lower()))\n",
    "\n",
    "#     # Require that most/all answer tokens are present\n",
    "#     return len(answer_tokens & chunk_tokens) / max(1, len(answer_tokens)) >= 0.8\n",
    "\n",
    "\n",
    "# from rapidfuzz import fuzz\n",
    "\n",
    "# def is_answer_in_chunk(chunk_text, answer_text, threshold=80):\n",
    "#     score = fuzz.partial_ratio(answer_text.lower(), chunk_text.lower())\n",
    "#     return score >= threshold\n",
    "\n",
    "def is_answer_in_chunk(answer_start, chunk_start, chunk_length):\n",
    "    if answer_start is None or chunk_start is None or chunk_length is None:\n",
    "        return False\n",
    "    return chunk_start <= answer_start < (chunk_start + chunk_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_top_k_accuracy(\n",
    "    df_queries,\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    cluster_labels,\n",
    "    top_n_clusters=2,\n",
    "    top_k_total=5\n",
    "):\n",
    "    # ✅ Compute centroids once\n",
    "    centroid_matrix, centroid_ids = compute_cluster_centroids(chunk_embeddings, cluster_labels)\n",
    "\n",
    "    y_true_doc = []\n",
    "    y_pred_doc = []\n",
    "\n",
    "    y_true_chunk = []\n",
    "    y_pred_chunk = []\n",
    "\n",
    "    for i, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "        query_emb = model.encode([row[\"question\"]])[0]\n",
    "        results = retrieve_top_chunks_by_cluster(\n",
    "            query_embedding=query_emb,\n",
    "            chunk_embeddings=chunk_embeddings,\n",
    "            df_chunks=df_chunks,\n",
    "            cluster_labels=cluster_labels,\n",
    "            centroid_matrix=centroid_matrix,\n",
    "            centroid_ids=centroid_ids,\n",
    "            top_n_clusters=top_n_clusters,\n",
    "            top_k_total=top_k_total\n",
    "        )\n",
    "\n",
    "        # Document-level\n",
    "        found_doc_id = any(row[\"context_id\"] == doc_id for doc_id in results[\"context_id\"])\n",
    "        y_true_doc.append(1)\n",
    "        y_pred_doc.append(1 if found_doc_id else 0)\n",
    "\n",
    "        if found_doc_id:\n",
    "            found_chunk_context = any(\n",
    "                is_answer_in_chunk(row[\"answer_start\"], chunk['chunk_start'], chunk['chunk_end'] - chunk['chunk_start'])\n",
    "                for _, chunk in results.iterrows()\n",
    "            )\n",
    "            y_true_chunk.append(1)\n",
    "            y_pred_chunk.append(1 if found_chunk_context else 0)\n",
    "        else:\n",
    "            y_true_chunk.append(1)\n",
    "            y_pred_chunk.append(0)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"doc_accuracy\": sum(y_pred_doc) / len(y_pred_doc),\n",
    "        \"chunk_accuracy\": sum(y_pred_chunk) / len(y_pred_chunk),\n",
    "        \"doc_precision\": precision_score(y_true_doc, y_pred_doc),\n",
    "        \"doc_recall\": recall_score(y_true_doc, y_pred_doc),\n",
    "        \"doc_f1\": f1_score(y_true_doc, y_pred_doc),\n",
    "        \"chunk_precision\": precision_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "        \"chunk_recall\": recall_score(y_true_chunk, y_pred_chunk),\n",
    "        \"chunk_f1\": f1_score(y_true_chunk, y_pred_chunk)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_top_k_accuracy_full(df_queries, chunk_embeddings, df_chunks, top_k_chunks=5):\n",
    "    y_true_doc = []\n",
    "    y_pred_doc = []\n",
    "\n",
    "    y_true_chunk = []\n",
    "    y_pred_chunk = []\n",
    "\n",
    "    for i, row in tqdm(df_queries.iterrows(), total=len(df_queries)):\n",
    "        query_emb = model.encode([row[\"question\"]])[0]\n",
    "        results = retrieve_top_chunks_full(\n",
    "            query_embedding=query_emb,\n",
    "            chunk_embeddings=chunk_embeddings,\n",
    "            df_chunks=df_chunks,\n",
    "            top_k_chunks=top_k_chunks\n",
    "        )\n",
    "\n",
    "        # Document-level\n",
    "        found_doc_id = any(row[\"context_id\"] == doc_id for doc_id in results[\"context_id\"])\n",
    "        y_true_doc.append(1)\n",
    "        y_pred_doc.append(1 if found_doc_id else 0)\n",
    "\n",
    "        # Chunk-level\n",
    "        if found_doc_id:\n",
    "            found_chunk_context = any(\n",
    "                is_answer_in_chunk(row[\"answer_start\"], chunk['chunk_start'], chunk['chunk_end'] - chunk['chunk_start'])\n",
    "                for _, chunk in results.iterrows()\n",
    "            )\n",
    "            y_true_chunk.append(1)\n",
    "            y_pred_chunk.append(1 if found_chunk_context else 0)\n",
    "        else:\n",
    "            y_true_chunk.append(1)\n",
    "            y_pred_chunk.append(0)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = {\n",
    "        \"doc_accuracy\": sum(y_pred_doc) / len(y_pred_doc),\n",
    "        \"chunk_accuracy\": sum(y_pred_chunk) / len(y_pred_chunk),\n",
    "        \"doc_precision\": precision_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "        \"doc_recall\": recall_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "        \"doc_f1\": f1_score(y_true_doc, y_pred_doc, zero_division=0),\n",
    "        \"chunk_precision\": precision_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "        \"chunk_recall\": recall_score(y_true_chunk, y_pred_chunk, zero_division=0),\n",
    "        \"chunk_f1\": f1_score(y_true_chunk, y_pred_chunk, zero_division=0)\n",
    "    }\n",
    "\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def minibatchkmeans_retrieval_evaluation(\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    df_queries,\n",
    "    n_clusters=20,\n",
    "    batch_size=500,\n",
    "    top_k_total=5,\n",
    "    init_fraction=0.1\n",
    "):\n",
    "    n_samples = chunk_embeddings.shape[0]\n",
    "    n_batches = int(np.ceil(n_samples / batch_size))\n",
    "\n",
    "    results = []\n",
    "\n",
    "    # --- Inicializáló klaszterezés ---\n",
    "    init_start = time.time()\n",
    "    init_size = max(1, int(n_samples * init_fraction))\n",
    "    kmeans = MiniBatchKMeans(n_clusters=n_clusters, random_state=42, batch_size=batch_size)\n",
    "    kmeans.partial_fit(chunk_embeddings[:init_size])\n",
    "    init_end = time.time()\n",
    "    init_time = init_end - init_start\n",
    "\n",
    "    print(\"Start batch processing...\")\n",
    "    for batch_idx in tqdm(range(1, n_batches + 1)):\n",
    "        start_idx = (batch_idx - 1) * batch_size\n",
    "        end_idx = min(batch_idx * batch_size, n_samples)\n",
    "        X_batch = chunk_embeddings[start_idx:end_idx]\n",
    "\n",
    "        # --- Online update ---\n",
    "        update_start = time.time()\n",
    "        kmeans.partial_fit(X_batch)\n",
    "        update_end = time.time()\n",
    "        update_time = update_end - update_start\n",
    "\n",
    "        # --- Klasztercímkék frissítése ---\n",
    "        labels = kmeans.predict(chunk_embeddings)\n",
    "\n",
    "        # --- Retrieval + pontosság ---\n",
    "        retrieval_start = time.time()\n",
    "        metrics = evaluate_top_k_accuracy(\n",
    "            df_queries=df_queries,\n",
    "            chunk_embeddings=chunk_embeddings,\n",
    "            df_chunks=df_chunks,\n",
    "            cluster_labels=labels,\n",
    "            top_n_clusters=5,\n",
    "            top_k_total=top_k_total\n",
    "        )\n",
    "        retrieval_end = time.time()\n",
    "        retrieval_time = retrieval_end - retrieval_start\n",
    "\n",
    "        results.append({\n",
    "            \"batch\": batch_idx,\n",
    "            \"init_time\": init_time if batch_idx == 1 else 0,\n",
    "            \"update_time\": update_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"metrics\": metrics,\n",
    "        })\n",
    "        print(f\"[Batch {batch_idx}/{n_batches}] Doc acc: {metrics['doc_accuracy']:.4f}, Chunk acc: {metrics['chunk_accuracy']:.4f}\")\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_kmeans_retrieval_evaluation(\n",
    "    chunk_embeddings,\n",
    "    df_chunks,\n",
    "    df_queries,\n",
    "    n_clusters=20,\n",
    "    batch_size=500,\n",
    "    top_k_total=5,\n",
    "    init_fraction=0.5,  # fraction of data used for initialization\n",
    "    max_clusters=None,\n",
    "    metric=\"cosine\",\n",
    "    new_cluster_threshold=None,\n",
    "    merge_threshold=None,\n",
    "    decay=None\n",
    "):\n",
    "    \"\"\"\n",
    "    OnlineKMeans clustering + retrieval evaluation on growing dataset.\n",
    "    Only evaluates on the chunks that have been clustered so far.\n",
    "    \"\"\"\n",
    "\n",
    "    n_samples = chunk_embeddings.shape[0]\n",
    "    init_size = int(n_samples * init_fraction)\n",
    "    remaining_size = n_samples - init_size\n",
    "\n",
    "    # --- Step 1: Initialization ---\n",
    "    print(f\"🔧 Using {init_fraction*100:.0f}% of data ({init_size} samples) for initialization\")\n",
    "    init_start = time.time()\n",
    "    okm = OnlineKMeans(\n",
    "        n_clusters=n_clusters,\n",
    "        max_clusters=max_clusters,\n",
    "        metric=metric,\n",
    "        new_cluster_threshold=new_cluster_threshold,\n",
    "        merge_threshold=merge_threshold,\n",
    "        random_state=42,\n",
    "        decay=decay\n",
    "    )\n",
    "    okm.partial_fit(chunk_embeddings[:init_size])\n",
    "    init_end = time.time()\n",
    "    init_time = init_end - init_start\n",
    "    print(f\"✅ Initialization done in {init_time:.4f} s\")\n",
    "\n",
    "    # --- Step 2: Online updates on the remaining data ---\n",
    "    results = []\n",
    "    for batch_idx in tqdm(range(1, int(np.ceil(remaining_size / batch_size)) + 1)):\n",
    "        start_idx = (batch_idx - 1) * batch_size\n",
    "        end_idx = min(batch_idx * batch_size, remaining_size)\n",
    "        batch_embeddings = chunk_embeddings[init_size + start_idx : init_size + end_idx]\n",
    "\n",
    "        # --- Online update ---\n",
    "        update_start = time.time()\n",
    "        okm.partial_fit(batch_embeddings)\n",
    "        update_end = time.time()\n",
    "        update_time = update_end - update_start\n",
    "\n",
    "        # --- Only evaluate on seen data so far ---\n",
    "        seen_end_idx = init_size + end_idx\n",
    "        seen_embeddings = chunk_embeddings[:seen_end_idx]\n",
    "        seen_df_chunks = df_chunks.iloc[:seen_end_idx].reset_index(drop=True)\n",
    "\n",
    "        # --- Predict cluster labels for seen data ---\n",
    "        labels_seen = okm.predict(seen_embeddings)\n",
    "\n",
    "        # --- Filter queries to only those with seen context_ids ---\n",
    "        df_queries_seen = df_queries[df_queries[\"context_id\"].isin(seen_df_chunks[\"context_id\"].unique())].reset_index(drop=True)\n",
    "        print(f\"df_queries_seen: {df_queries_seen.shape[0]}, seen_df_chunks: {seen_df_chunks.shape[0]}\")\n",
    "\n",
    "        # --- Retrieval accuracy ---\n",
    "        retrieval_start = time.time()\n",
    "        metrics = evaluate_top_k_accuracy(\n",
    "            df_queries=df_queries_seen,\n",
    "            chunk_embeddings=seen_embeddings,\n",
    "            df_chunks=seen_df_chunks,\n",
    "            cluster_labels=labels_seen,\n",
    "            top_n_clusters=5,\n",
    "            top_k_total=top_k_total\n",
    "        )\n",
    "        retrieval_end = time.time()\n",
    "        retrieval_time = retrieval_end - retrieval_start\n",
    "\n",
    "        results.append({\n",
    "            \"batch\": batch_idx,\n",
    "            \"init_time\": init_time if batch_idx == 1 else 0,\n",
    "            \"update_time\": update_time,\n",
    "            \"retrieval_time\": retrieval_time,\n",
    "            \"metrics\": metrics,\n",
    "            \"n_clusters\": len(okm.centroids)\n",
    "        })\n",
    "\n",
    "        print(f\"[Batch {batch_idx}] Seen chunks: {seen_end_idx}, Doc acc: {metrics['doc_accuracy']:.4f}, Chunk acc: {metrics['chunk_accuracy']:.4f}, Clusters: {len(okm.centroids)}\")\n",
    "\n",
    "    return pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_excel(\"./data/labelled/squad_train_v2_semantic_chunking_clustered.xlsx\")\n",
    "df_val = pd.read_excel(\"./data/labelled/squad_val_v2_semantic_chunking_clustered.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"./data/prepared/squad_train_v2_queries.xlsx\")\n",
    "df_queries_train = df_queries_train[df_queries_train[\"context_id\"].isin(df_train[\"context_id\"].unique())].reset_index(drop=True)\n",
    "\n",
    "X_train = np.load(\"./data/labelled/squad_train_v2_semantic_chunking_clustered.npy\")\n",
    "X_val = np.load(\"./data/labelled/squad_val_v2_semantic_chunking_clustered.npy\")\n",
    "df_queries_val = pd.read_excel(\"./data/prepared/squad_val_v2_queries.xlsx\")\n",
    "\n",
    "labels_train = df_train[\"cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2201, 10)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2329, 4)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In what year did the team lead by Knute Rockne win the Rose Bowl?'"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_queries_train.loc[100, \"question\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'In what year did the team lead by Knute Rockne win the Rose Bowl?'\n",
    "query_emb = model.encode([query])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 0.01423 seconds\n",
      "Cluster-based retrieval:\n",
      "[\"'96, Heisman Trophy winner Ty Detmer '90, and two-time Super Bowl winner Jim McMahon.\", '23-year-old Candice Glover won the season with Kree Harrison taking the runner-up spot.', \"BYU also claims notable professional football players including two-time NFL MVP and Super Bowl MVP and Pro Football Hall of Fame quarterback Steve Young '84 & J.D.\"]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "top_chunks_cluster = retrieve_top_chunks_by_cluster(\n",
    "    query_embedding=query_emb,\n",
    "    chunk_embeddings=X_train,\n",
    "    df_chunks=df_train,\n",
    "    cluster_labels=labels_train,\n",
    "    top_n_clusters=3,\n",
    "    top_k_total=3\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Runtime: {elapsed_time:.5f} seconds\")\n",
    "print(\"Cluster-based retrieval:\")\n",
    "print(top_chunks_cluster['chunk_embed_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Runtime: 0.00212 seconds\n",
      "Full retrieval:\n",
      "[\"'96, Heisman Trophy winner Ty Detmer '90, and two-time Super Bowl winner Jim McMahon.\", '23-year-old Candice Glover won the season with Kree Harrison taking the runner-up spot.', \"BYU also claims notable professional football players including two-time NFL MVP and Super Bowl MVP and Pro Football Hall of Fame quarterback Steve Young '84 & J.D.\"]\n"
     ]
    }
   ],
   "source": [
    "start_time = time.time()\n",
    "\n",
    "top_chunks_full = retrieve_top_chunks_full(\n",
    "    query_embedding=query_emb,\n",
    "    chunk_embeddings=X_train,\n",
    "    df_chunks=df_train,\n",
    "    top_k_chunks=3\n",
    ")\n",
    "\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"Runtime: {elapsed_time:.5f} seconds\")\n",
    "print(\"Full retrieval:\")\n",
    "print(top_chunks_full['chunk_embed_text'].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation for retrival with cluster centroids vs full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_semantic_train = np.load(\"../data/labelled/squad_train_v2_semantic_chunking_clustered.npy\")\n",
    "df_semantic_train = pd.read_excel(\"../data/labelled/squad_train_v2_semantic_chunking_clustered.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"../data/prepared/squad_train_v2_queries.xlsx\")\n",
    "\n",
    "labels_train = df_semantic_train[\"cluster\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-3 chunks in Top-5 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [31:29<00:00, 46.36it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-3 chunks in Top-10 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [35:34<00:00, 41.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-3 chunks in Top-20 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [41:19<00:00, 35.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-3 chunks in Top-35 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [50:06<00:00, 29.14it/s]  \n",
      "100%|██████████| 87599/87599 [3:02:51<00:00,  7.98it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-5 chunks in Top-5 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [32:44<00:00, 44.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-5 chunks in Top-10 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [36:02<00:00, 40.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-5 chunks in Top-20 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [41:59<00:00, 34.77it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-5 chunks in Top-35 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [50:30<00:00, 28.91it/s]  \n",
      "100%|██████████| 87599/87599 [2:55:05<00:00,  8.34it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-12 chunks in Top-5 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [36:41<00:00, 39.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-12 chunks in Top-10 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [40:05<00:00, 36.42it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-12 chunks in Top-20 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [45:56<00:00, 31.78it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-12 chunks in Top-35 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [58:24<00:00, 25.00it/s]  \n",
      "100%|██████████| 87599/87599 [3:17:52<00:00,  7.38it/s]   \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-25 chunks in Top-5 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [55:07<00:00, 26.48it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-25 chunks in Top-10 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [47:39<00:00, 30.64it/s]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-25 chunks in Top-20 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [1:04:50<00:00, 22.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating: Top-25 chunks in Top-35 clusters\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [1:11:56<00:00, 20.29it/s]\n",
      "  8%|▊         | 7101/87599 [15:35<2:56:50,  7.59it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 25\u001b[39m\n\u001b[32m     22\u001b[39m     results_df_centroid.to_excel(\u001b[33m\"\u001b[39m\u001b[33m../data/results/hyperparameter_tuning_centroid_vs_full/centroid_results.xlsx\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     24\u001b[39m start_full = time.time()\n\u001b[32m---> \u001b[39m\u001b[32m25\u001b[39m full_metrics = \u001b[43mevaluate_top_k_accuracy_full\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf_queries_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_semantic_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdf_semantic_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtop_k_chunks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtop_k\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     26\u001b[39m end_full = time.time()\n\u001b[32m     27\u001b[39m results_full.append({\n\u001b[32m     28\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtop_k\u001b[39m\u001b[33m\"\u001b[39m: top_k,\n\u001b[32m     29\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfull_metrics\u001b[39m\u001b[33m\"\u001b[39m: full_metrics,\n\u001b[32m     30\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfull_time\u001b[39m\u001b[33m\"\u001b[39m: end_full - start_full\n\u001b[32m     31\u001b[39m })\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 9\u001b[39m, in \u001b[36mevaluate_top_k_accuracy_full\u001b[39m\u001b[34m(df_queries, chunk_embeddings, df_chunks, top_k_chunks)\u001b[39m\n\u001b[32m      6\u001b[39m y_pred_chunk = []\n\u001b[32m      8\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i, row \u001b[38;5;129;01min\u001b[39;00m tqdm(df_queries.iterrows(), total=\u001b[38;5;28mlen\u001b[39m(df_queries)):\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m     query_emb = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencode\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mrow\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mquestion\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m[\u001b[32m0\u001b[39m]\n\u001b[32m     10\u001b[39m     results = retrieve_top_chunks_full(\n\u001b[32m     11\u001b[39m         query_embedding=query_emb,\n\u001b[32m     12\u001b[39m         chunk_embeddings=chunk_embeddings,\n\u001b[32m     13\u001b[39m         df_chunks=df_chunks,\n\u001b[32m     14\u001b[39m         top_k_chunks=top_k_chunks\n\u001b[32m     15\u001b[39m     )\n\u001b[32m     17\u001b[39m     \u001b[38;5;66;03m# Document-level\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\torch\\utils\\_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\sentence_transformers\\SentenceTransformer.py:1052\u001b[39m, in \u001b[36mSentenceTransformer.encode\u001b[39m\u001b[34m(self, sentences, prompt_name, prompt, batch_size, show_progress_bar, output_value, precision, convert_to_numpy, convert_to_tensor, device, normalize_embeddings, truncate_dim, pool, chunk_size, **kwargs)\u001b[39m\n\u001b[32m   1049\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m device \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m   1050\u001b[39m     device = \u001b[38;5;28mself\u001b[39m.device\n\u001b[32m-> \u001b[39m\u001b[32m1052\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1054\u001b[39m truncate_dim = truncate_dim \u001b[38;5;28;01mif\u001b[39;00m truncate_dim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.truncate_dim\n\u001b[32m   1056\u001b[39m all_embeddings = []\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1369\u001b[39m, in \u001b[36mModule.to\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1366\u001b[39m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1367\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1369\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "    \u001b[31m[... skipping similar frames: Module._apply at line 928 (1 times)]\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\torch\\nn\\modules\\module.py:928\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    926\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m recurse:\n\u001b[32m    927\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.children():\n\u001b[32m--> \u001b[39m\u001b[32m928\u001b[39m         \u001b[43mmodule\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    930\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[32m    931\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m torch._has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[32m    932\u001b[39m         \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[32m    933\u001b[39m         \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    938\u001b[39m         \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[32m    939\u001b[39m         \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\torch\\nn\\modules\\module.py:955\u001b[39m, in \u001b[36mModule._apply\u001b[39m\u001b[34m(self, fn, recurse)\u001b[39m\n\u001b[32m    951\u001b[39m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[32m    952\u001b[39m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[32m    953\u001b[39m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[32m    954\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m--> \u001b[39m\u001b[32m955\u001b[39m     param_applied = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    956\u001b[39m p_should_use_set_data = compute_should_use_set_data(param, param_applied)\n\u001b[32m    958\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01m_subclasses\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfake_tensor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FakeTensor\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1355\u001b[39m, in \u001b[36mModule.to.<locals>.convert\u001b[39m\u001b[34m(t)\u001b[39m\n\u001b[32m   1348\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t.dim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[32m4\u001b[39m, \u001b[32m5\u001b[39m):\n\u001b[32m   1349\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m t.to(\n\u001b[32m   1350\u001b[39m             device,\n\u001b[32m   1351\u001b[39m             dtype \u001b[38;5;28;01mif\u001b[39;00m t.is_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t.is_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m   1352\u001b[39m             non_blocking,\n\u001b[32m   1353\u001b[39m             memory_format=convert_to_format,\n\u001b[32m   1354\u001b[39m         )\n\u001b[32m-> \u001b[39m\u001b[32m1355\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1357\u001b[39m \u001b[43m        \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1358\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1359\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1360\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m   1361\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(e) == \u001b[33m\"\u001b[39m\u001b[33mCannot copy out of meta tensor; no data!\u001b[39m\u001b[33m\"\u001b[39m:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "top_ks = [3, 5, 12, 25]\n",
    "top_n_clusters = [5, 10, 20, 35]\n",
    "\n",
    "results_centroid = []\n",
    "results_full = []\n",
    "for top_k in top_ks:\n",
    "    for top_n_cluster in top_n_clusters:\n",
    "        print(f\"Evaluating: Top-{top_k} chunks in Top-{top_n_cluster} clusters\")\n",
    "        \n",
    "        start_centroid = time.time()\n",
    "        centroid_metrics = evaluate_top_k_accuracy(df_queries_train, X_semantic_train, df_semantic_train, labels_train, top_n_clusters=top_n_cluster, top_k_total=top_k)\n",
    "        end_centroid = time.time()\n",
    "        \n",
    "        results_centroid.append({\n",
    "            \"top_k\": top_k,\n",
    "            \"top_n_clusters\": top_n_cluster,\n",
    "            \"centroid_metrics\": centroid_metrics,\n",
    "            \"centroid_time\": end_centroid - start_centroid\n",
    "        })\n",
    "        \n",
    "        results_df_centroid = pd.DataFrame(results_centroid)\n",
    "        results_df_centroid.to_excel(\"../data/results/hyperparameter_tuning_centroid_vs_full/centroid_results.xlsx\")\n",
    "        \n",
    "    start_full = time.time()\n",
    "    full_metrics = evaluate_top_k_accuracy_full(df_queries_train, X_semantic_train, df_semantic_train, top_k_chunks=top_k)\n",
    "    end_full = time.time()\n",
    "    results_full.append({\n",
    "        \"top_k\": top_k,\n",
    "        \"full_metrics\": full_metrics,\n",
    "        \"full_time\": end_full - start_full\n",
    "    })\n",
    "    results_df_full = pd.DataFrame(results_full)\n",
    "    results_df_full.to_excel(\"../data/results/hyperparameter_tuning_centroid_vs_full/full_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [3:28:17<00:00,  7.01it/s]   \n"
     ]
    }
   ],
   "source": [
    "start_full = time.time()\n",
    "full_metrics = evaluate_top_k_accuracy_full(df_queries_train, X_semantic_train, df_semantic_train, top_k_chunks=25)\n",
    "end_full = time.time()\n",
    "results_full.append({\n",
    "    \"top_k\": 25,\n",
    "    \"full_metrics\": full_metrics,\n",
    "    \"full_time\": end_full - start_full\n",
    "})\n",
    "results_df_full = pd.DataFrame(results_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>top_k</th>\n",
       "      <th>full_metrics</th>\n",
       "      <th>full_time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>{'doc_accuracy': 0.8036735579173279, 'chunk_ac...</td>\n",
       "      <td>10972.379970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>{'doc_accuracy': 0.8522357561159374, 'chunk_ac...</td>\n",
       "      <td>10506.523075</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>12</td>\n",
       "      <td>{'doc_accuracy': 0.9135035788079773, 'chunk_ac...</td>\n",
       "      <td>11872.974770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25</td>\n",
       "      <td>{'doc_accuracy': 0.9460039498167787, 'chunk_ac...</td>\n",
       "      <td>12497.922858</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   top_k                                       full_metrics     full_time\n",
       "0      3  {'doc_accuracy': 0.8036735579173279, 'chunk_ac...  10972.379970\n",
       "1      5  {'doc_accuracy': 0.8522357561159374, 'chunk_ac...  10506.523075\n",
       "2     12  {'doc_accuracy': 0.9135035788079773, 'chunk_ac...  11872.974770\n",
       "3     25  {'doc_accuracy': 0.9460039498167787, 'chunk_ac...  12497.922858"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df_full.to_excel(\"../data/results/hyperparameter_tuning_centroid_vs_full/full_results.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate retrieval with MiniBatchKMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_semantic_train = np.load(\"./data/tensors/squad_train_v2_semantic_chunking.npy\")\n",
    "df_semantic_train = pd.read_excel(\"./data/prepared/squad_train_v2_semantic_chunking.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"./data/prepared/squad_train_v2_queries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Futtatás ---\n",
    "results_df = minibatchkmeans_retrieval_evaluation(\n",
    "    chunk_embeddings=X_semantic_train,\n",
    "    df_chunks=df_semantic_train,\n",
    "    df_queries=df_queries_train,\n",
    "    n_clusters=160,\n",
    "    batch_size=1000,\n",
    "    top_k_total=5,\n",
    "    init_fraction=0.5\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Plotolás: Pontosság ---\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"doc_accuracy\"], label=\"Doc Accuracy\", marker='o')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"chunk_accuracy\"], label=\"Chunk Accuracy\", marker='s')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Retrieval pontosság batchenként (Online KMeans)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plotolás: Futásidők ---\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"init_time\"], label=\"Init time\", marker='o')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"update_time\"], label=\"Update time\", marker='s')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"retrieval_time\"], label=\"Retrieval time\", marker='^')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"Futásidők batchenként (Online KMeans)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate retrieval with online clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data\n",
    "X_semantic_train = np.load(\"../data/tensors/squad_train_v2_semantic_chunking.npy\")\n",
    "df_semantic_train = pd.read_excel(\"../data/prepared/squad_train_v2_semantic_chunking.xlsx\")\n",
    "df_queries_train = pd.read_excel(\"../data/prepared/squad_train_v2_queries.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔧 Using 50% of data (42003 samples) for initialization\n",
      "✅ Initialization done in 38.6944 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/22 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "df_queries_seen: 48817, seen_df_chunks: 44003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 48817/48817 [16:11<00:00, 50.23it/s]\n",
      "  5%|▍         | 1/22 [16:12<5:40:28, 972.77s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 1] Seen chunks: 44003, Doc acc: 0.7526, Chunk acc: 0.6831, Clusters: 524\n",
      "df_queries_seen: 50578, seen_df_chunks: 46003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50578/50578 [19:55<00:00, 42.32it/s]\n",
      "  9%|▉         | 2/22 [36:08<6:08:02, 1104.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 2] Seen chunks: 46003, Doc acc: 0.7504, Chunk acc: 0.6803, Clusters: 524\n",
      "df_queries_seen: 52477, seen_df_chunks: 48003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 52477/52477 [19:34<00:00, 44.70it/s]\n",
      " 14%|█▎        | 3/22 [55:43<5:59:53, 1136.52s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 3] Seen chunks: 48003, Doc acc: 0.7481, Chunk acc: 0.6784, Clusters: 526\n",
      "df_queries_seen: 54231, seen_df_chunks: 50003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 54231/54231 [19:52<00:00, 45.48it/s]\n",
      " 18%|█▊        | 4/22 [1:15:37<5:47:41, 1158.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 4] Seen chunks: 50003, Doc acc: 0.7456, Chunk acc: 0.6742, Clusters: 544\n",
      "df_queries_seen: 56119, seen_df_chunks: 52003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 56119/56119 [20:09<00:00, 46.39it/s]\n",
      " 23%|██▎       | 5/22 [1:35:47<5:33:39, 1177.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 5] Seen chunks: 52003, Doc acc: 0.7438, Chunk acc: 0.6724, Clusters: 544\n",
      "df_queries_seen: 58097, seen_df_chunks: 54003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 58097/58097 [21:08<00:00, 45.82it/s]\n",
      " 27%|██▋       | 6/22 [1:56:57<5:22:20, 1208.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 6] Seen chunks: 54003, Doc acc: 0.7416, Chunk acc: 0.6703, Clusters: 544\n",
      "df_queries_seen: 59930, seen_df_chunks: 56003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 59930/59930 [22:23<00:00, 44.61it/s]\n",
      " 32%|███▏      | 7/22 [2:19:22<5:13:18, 1253.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 7] Seen chunks: 56003, Doc acc: 0.7393, Chunk acc: 0.6687, Clusters: 544\n",
      "df_queries_seen: 61832, seen_df_chunks: 58003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 61832/61832 [22:59<00:00, 44.82it/s]\n",
      " 36%|███▋      | 8/22 [2:42:23<5:01:54, 1293.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 8] Seen chunks: 58003, Doc acc: 0.7369, Chunk acc: 0.6663, Clusters: 544\n",
      "df_queries_seen: 63816, seen_df_chunks: 60003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 63816/63816 [23:57<00:00, 44.40it/s]\n",
      " 41%|████      | 9/22 [3:06:21<4:50:09, 1339.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 9] Seen chunks: 60003, Doc acc: 0.7358, Chunk acc: 0.6651, Clusters: 550\n",
      "df_queries_seen: 65742, seen_df_chunks: 62003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 65742/65742 [24:42<00:00, 44.33it/s]\n",
      " 45%|████▌     | 10/22 [3:31:05<4:36:47, 1383.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 10] Seen chunks: 62003, Doc acc: 0.7341, Chunk acc: 0.6637, Clusters: 550\n",
      "df_queries_seen: 67503, seen_df_chunks: 64003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 67503/67503 [25:50<00:00, 43.54it/s]\n",
      " 50%|█████     | 11/22 [3:56:57<4:23:08, 1435.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 11] Seen chunks: 64003, Doc acc: 0.7335, Chunk acc: 0.6626, Clusters: 551\n",
      "df_queries_seen: 69330, seen_df_chunks: 66003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 69330/69330 [26:16<00:00, 43.99it/s]\n",
      " 55%|█████▍    | 12/22 [4:23:15<4:06:25, 1478.56s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 12] Seen chunks: 66003, Doc acc: 0.7310, Chunk acc: 0.6606, Clusters: 551\n",
      "df_queries_seen: 71487, seen_df_chunks: 68003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 71487/71487 [29:58<00:00, 39.74it/s]\n",
      " 59%|█████▉    | 13/22 [4:53:16<3:56:26, 1576.23s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 13] Seen chunks: 68003, Doc acc: 0.7297, Chunk acc: 0.6592, Clusters: 551\n",
      "df_queries_seen: 73342, seen_df_chunks: 70003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 73342/73342 [28:31<00:00, 42.85it/s]\n",
      " 64%|██████▎   | 14/22 [5:21:50<3:35:43, 1617.89s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 14] Seen chunks: 70003, Doc acc: 0.7287, Chunk acc: 0.6580, Clusters: 552\n",
      "df_queries_seen: 75361, seen_df_chunks: 72003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 75361/75361 [32:28<00:00, 38.67it/s]\n",
      " 68%|██████▊   | 15/22 [5:54:20<3:20:27, 1718.17s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 15] Seen chunks: 72003, Doc acc: 0.7256, Chunk acc: 0.6553, Clusters: 552\n",
      "df_queries_seen: 77458, seen_df_chunks: 74003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 77458/77458 [31:24<00:00, 41.11it/s]\n",
      " 73%|███████▎  | 16/22 [6:25:46<2:56:51, 1768.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 16] Seen chunks: 74003, Doc acc: 0.7246, Chunk acc: 0.6548, Clusters: 554\n",
      "df_queries_seen: 79607, seen_df_chunks: 76003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 79607/79607 [31:25<00:00, 42.22it/s]\n",
      " 77%|███████▋  | 17/22 [6:57:13<2:30:21, 1804.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 17] Seen chunks: 76003, Doc acc: 0.7247, Chunk acc: 0.6551, Clusters: 554\n",
      "df_queries_seen: 81585, seen_df_chunks: 78003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 81585/81585 [33:05<00:00, 41.08it/s]\n",
      " 82%|████████▏ | 18/22 [7:30:21<2:03:57, 1859.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 18] Seen chunks: 78003, Doc acc: 0.7230, Chunk acc: 0.6536, Clusters: 554\n",
      "df_queries_seen: 83407, seen_df_chunks: 80003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83407/83407 [33:33<00:00, 41.43it/s]\n",
      " 86%|████████▋ | 19/22 [8:03:56<1:35:18, 1906.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 19] Seen chunks: 80003, Doc acc: 0.7227, Chunk acc: 0.6532, Clusters: 555\n",
      "df_queries_seen: 85432, seen_df_chunks: 82003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 85432/85432 [34:35<00:00, 41.16it/s]\n",
      " 91%|█████████ | 20/22 [8:38:33<1:05:15, 1957.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 20] Seen chunks: 82003, Doc acc: 0.7207, Chunk acc: 0.6512, Clusters: 555\n",
      "df_queries_seen: 87594, seen_df_chunks: 84003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87594/87594 [35:20<00:00, 41.30it/s]\n",
      " 95%|█████████▌| 21/22 [9:13:56<33:27, 2007.11s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 21] Seen chunks: 84003, Doc acc: 0.7193, Chunk acc: 0.6502, Clusters: 555\n",
      "df_queries_seen: 87599, seen_df_chunks: 84007\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 87599/87599 [34:47<00:00, 41.96it/s]\n",
      "100%|██████████| 22/22 [9:48:45<00:00, 1605.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Batch 22] Seen chunks: 84007, Doc acc: 0.7193, Chunk acc: 0.6502, Clusters: 555\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "results_df = online_kmeans_retrieval_evaluation(\n",
    "    chunk_embeddings=X_semantic_train,\n",
    "    df_chunks=df_semantic_train,\n",
    "    df_queries=df_queries_train,\n",
    "    n_clusters=500,\n",
    "    max_clusters=2000,\n",
    "    batch_size=2000,\n",
    "    top_k_total=5,\n",
    "    metric=\"cosine\",\n",
    "    init_fraction=0.5,\n",
    "    merge_threshold=0.08,    \n",
    "    decay=1.0,\n",
    "    new_cluster_threshold=0.8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_excel(\"../data/results/onlinekmeans_v2.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'doc_accuracy'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3812\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3811\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3812\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3813\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:167\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/index.pyx:196\u001b[39m, in \u001b[36mpandas._libs.index.IndexEngine.get_loc\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7088\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mpandas/_libs/hashtable_class_helper.pxi:7096\u001b[39m, in \u001b[36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[39m\u001b[34m()\u001b[39m\n",
      "\u001b[31mKeyError\u001b[39m: 'doc_accuracy'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mKeyError\u001b[39m                                  Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[18]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;66;03m# --- Plot Accuracy ---\u001b[39;00m\n\u001b[32m      2\u001b[39m plt.figure(figsize=(\u001b[32m10\u001b[39m, \u001b[32m5\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m plt.plot(results_df[\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m], \u001b[43mresults_df\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mdoc_accuracy\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m]\u001b[49m, marker=\u001b[33m\"\u001b[39m\u001b[33mo\u001b[39m\u001b[33m\"\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mDoc Accuracy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      4\u001b[39m plt.plot(results_df[\u001b[33m\"\u001b[39m\u001b[33mbatch\u001b[39m\u001b[33m\"\u001b[39m], results_df[\u001b[33m\"\u001b[39m\u001b[33mchunk_accuracy\u001b[39m\u001b[33m\"\u001b[39m], marker=\u001b[33m\"\u001b[39m\u001b[33ms\u001b[39m\u001b[33m\"\u001b[39m, label=\u001b[33m\"\u001b[39m\u001b[33mChunk Accuracy\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m plt.xlabel(\u001b[33m\"\u001b[39m\u001b[33mBatch\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\pandas\\core\\frame.py:4113\u001b[39m, in \u001b[36mDataFrame.__getitem__\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   4111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.columns.nlevels > \u001b[32m1\u001b[39m:\n\u001b[32m   4112\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._getitem_multilevel(key)\n\u001b[32m-> \u001b[39m\u001b[32m4113\u001b[39m indexer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[32m   4115\u001b[39m     indexer = [indexer]\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\CZB3BP\\.conda\\envs\\szakdoga\\Lib\\site-packages\\pandas\\core\\indexes\\base.py:3819\u001b[39m, in \u001b[36mIndex.get_loc\u001b[39m\u001b[34m(self, key)\u001b[39m\n\u001b[32m   3814\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[32m   3815\u001b[39m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc.Iterable)\n\u001b[32m   3816\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[32m   3817\u001b[39m     ):\n\u001b[32m   3818\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[32m-> \u001b[39m\u001b[32m3819\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01merr\u001b[39;00m\n\u001b[32m   3820\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m   3821\u001b[39m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[32m   3822\u001b[39m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[32m   3823\u001b[39m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[32m   3824\u001b[39m     \u001b[38;5;28mself\u001b[39m._check_indexing_error(key)\n",
      "\u001b[31mKeyError\u001b[39m: 'doc_accuracy'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# --- Plot Accuracy ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"doc_accuracy\"], marker=\"o\", label=\"Doc Accuracy\")\n",
    "plt.plot(results_df[\"batch\"], results_df[\"chunk_accuracy\"], marker=\"s\", label=\"Chunk Accuracy\")\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"📊 Retrieval Accuracy per Batch (OnlineKMeans)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot Runtimes ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"update_time\"], label=\"Update Time\", marker='o')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"retrieval_time\"], label=\"Retrieval Time\", marker='s')\n",
    "plt.plot(results_df[\"batch\"], results_df[\"prediction_time\"], label=\"Prediction Time\", marker='^')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"Time (s)\")\n",
    "plt.title(\"⚙️ Runtime per Batch (OnlineKMeans)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# --- Plot number of clusters (optional) ---\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(results_df[\"batch\"], results_df[\"n_clusters\"], label=\"Number of Clusters\", marker='o', color='purple')\n",
    "plt.xlabel(\"Batch\")\n",
    "plt.ylabel(\"# Clusters\")\n",
    "plt.title(\"📈 Cluster Count Evolution (OnlineKMeans)\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "szakdoga",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
